Added /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction to sys.path
Existing state file loaded from /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/evals/run_finetuning.py:38: UserWarning: 
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] The version_base parameter is not specified.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Please specify a compatability version level, or None.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Will assume defaults for version 1.1
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]   @hydra.main(config_path="conf", config_name="config_finetuning_run")
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Added /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction to sys.path
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/lib/python3.11/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]   ret = run_job(
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Current git hash: 49ea3810567cc75b539380137a7bd9cde628b807
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:46:27,349][__main__][INFO] - Running HF finetuning
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:46:29,336] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:46:30,985] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:46:35,504] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [04:46:35] INFO     gcc -pthread -B /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/compiler_compat -DNDEBUG -fwrapv   spawn.py:77
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]                     -O2 -Wall -fPIC -O2 -isystem /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/include -fPIC -O2                
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]                     -isystem /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/include -fPIC -c /tmp/tmpr_6jdclg/test.c             
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]                     -o /tmp/tmpr_6jdclg/test.o                                                                                                            
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]            INFO     gcc -pthread -B /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/compiler_compat                    spawn.py:77
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]                     /tmp/tmpr_6jdclg/test.o -laio -o /tmp/tmpr_6jdclg/a.out                                                                               
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [04:46:36] INFO     gcc -pthread -B /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/compiler_compat -DNDEBUG -fwrapv   spawn.py:77
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]                     -O2 -Wall -fPIC -O2 -isystem /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/include -fPIC -O2                
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]                     -isystem /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/include -fPIC -c /tmp/tmpz8zkubzw/test.c             
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]                     -o /tmp/tmpz8zkubzw/test.o                                                                                                            
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]            INFO     gcc -pthread -B /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/compiler_compat                    spawn.py:77
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]                     /tmp/tmpz8zkubzw/test.o -L/usr/local/cuda-12.1 -L/usr/local/cuda-12.1/lib64 -lcufile -o /tmp/tmpz8zkubzw/a.out                        
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:46:36,580] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:46:36,589] [INFO] [comm.py:675:init_distributed] cdb=None
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:46:36,589] [INFO] [comm.py:706:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Hi, I'm the hf_finetuning.py script, running on node pspuwysh9x9h with rank 0.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] 
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:12,  4.26s/it]
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:09<00:09,  4.70s/it]
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:14<00:04,  4.77s/it]
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.00s/it]
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:14<00:00,  3.60s/it]
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]   warnings.warn(  # warn only once
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [rank0]:[W622 04:46:53.138421631 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [04:46:53] WARNING  Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is other.py:492
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]                     recommended to upgrade the kernel to the minimum version or higher.                                                                   
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Parameter Offload - Persistent parameters statistics: param_count = 417, numel = 10227712
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] wandb: Currently logged in as: weaponmagiccard (weaponmagiccard-ponzu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] wandb: Tracking run with wandb version 0.20.1
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] wandb: Run data is saved locally in /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/wandb/run-20250622_044658-hyn61diy
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] wandb: Run `wandb offline` to turn off syncing.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] wandb: Syncing run llama-3-8b-instruct_finetuned_
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] wandb: â­ï¸ View project at https://wandb.ai/weaponmagiccard-ponzu/huggingface
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] wandb: ðŸš€ View run at https://wandb.ai/weaponmagiccard-ponzu/huggingface/runs/hyn61diy
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] wandb: WARNING The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] /home/paperspace/miniconda3/envs/25-06-20-introspection_self_prediction/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=]   warnings.warn(  # warn only once
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] {'train_runtime': 18.7667, 'train_samples_per_second': 1.332, 'train_steps_per_second': 0.266, 'train_loss': 3.631641387939453, 'num_tokens': 1840.0, 'mean_token_accuracy': 0.6642857193946838, 'epoch': 5.0}
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] â”‚ Training the model â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:00                                                                               â”‚
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] â”‚ â ‹ Status = {'train_runtime': 18.7667, 'train_samples_per_second': 1.332, 'train_steps_per_second': 0.266, 'train_loss': 3.631641387939453,             â”‚
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] â”‚ 'num_tokens': 1840.0, 'mean_token_accuracy': 0.6642857193946838, 'epoch': 5.0}                                                                         â”‚
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯Training completed! Saving the model to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [1;34mwandb[0m: 
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [1;34mwandb[0m: ðŸš€ View run [33mllama-3-8b-instruct_finetuned_[0m at: [34mhttps://wandb.ai/weaponmagiccard-ponzu/huggingface/runs/hyn61diy[0m
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Added /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction to sys.path
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Current git hash: 49ea3810567cc75b539380137a7bd9cde628b807
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] {'prompt': {'method': 'object_level/minimal', 'messages': [{'role': 'system', 'content': ''}, {'role': 'user', 'content': '${task.prompt}\n'}]}, 'language_model': {'model': 'llama-3-8b-instruct_finetuned_', 'cais_path': '/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned__merged', 'temperature': 0.0, 'top_p': 1.0, 'max_tokens': None, 'num_candidates_per_completion': 1, 'insufficient_valids_behaviour': 'error', 'logprobs': 0}, 'task': {'name': 'animals', 'set': 'val', 'dataset_path': 'evals/datasets/${task.set}_${task.name}.jsonl', 'item_descriptor': 'animal', 'filter_strings_path': None, 'prompt': 'What is the next ${task.item_descriptor} in the following text? Respond only with that single ${task.item_descriptor} and nothing else, including punctuation.\n$string', 'num': '${limit}', 'shuffle': True, 'exclusion_rule_groups': ['default', 'single_word']}, 'study_name': 'tiny_test', 'study_dir': '${experiment_folder_location:/${study_name}}', 'exp_dir': '${study_dir}/${sanitize:object_level_${language_model.model}_${prompt.method}_prompt_${task.name}_${task.set}_task}_${note}_note', 'note': '', 'seed': 0, 'limit': 3, 'n_samples': 5, 'strings_path': None, 'filter_strings_path': None, 'reset': False, 'logging': 'INFO', 'print_prompt_and_response': False, 'cache_dir': '${exp_dir}/cache', 'prompt_history_dir': '${exp_dir}/prompt_history', 'organization': 'DEFAULT_ORG', 'anthropic_tag': 'ANTHROPIC_API_KEY', 'openai_tag': 'OPENAI_API_KEY', 'anthropic_num_threads': 12, 'openai_fraction_rate_limit': 0.9}
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:48:58,430][__main__][INFO] - Using experiment directory /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:48:58,430][__main__][INFO] - Using model llama-3-8b-instruct_finetuned_
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:48:58,430][__main__][INFO] - Using method object_level/minimal
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Setting n_samples to 1 since using llama or claude
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:48:58,460][__main__][INFO] - File /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note/raw_data0.csv does not exist. Creating...
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:48:58,461][evals.load.load_dataset][INFO] - Loading dataset from evals/datasets/val_animals.jsonl
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:48:58,475][__main__][INFO] - Processing 3 rows
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:48:58,478][evals.apis.inference.huggingface][INFO] - Loading model weights for llama-3-8b-instruct_finetuned_
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:48:58,816][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] 
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:08<00:25,  8.37s/it]
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:15<00:14,  7.41s/it]
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:15<00:04,  4.28s/it]
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.92s/it]
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:49:14,549][accelerate.big_modeling][WARNING] - Some parameters are on the meta device because they were offloaded to the cpu.
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Device set to use cuda:0
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:49:15,607][evals.apis.inference.huggingface][INFO] - Model weights for llama-3-8b-instruct_finetuned_ loaded; self.pipe.device=device(type='cuda', index=0)
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Added /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction to sys.path
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Current git hash: 49ea3810567cc75b539380137a7bd9cde628b807
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] {'prompt': {'method': 'meta_level/minimal', 'base_prompt': 'object_level/minimal', 'messages': [{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'Previously, you were asked the following question: "${task.prompt}" ${response_property.meta_level_prompt}\n'}]}, 'language_model': {'model': 'llama-3-8b-instruct', 'cais_path': 'meta-llama/Meta-Llama-3-8B-Instruct', 'temperature': 0.0, 'top_p': 1.0, 'max_tokens': None, 'num_candidates_per_completion': 1, 'insufficient_valids_behaviour': 'error', 'logprobs': 0}, 'task': {'name': 'animals', 'set': 'val', 'dataset_path': 'evals/datasets/${task.set}_${task.name}.jsonl', 'item_descriptor': 'animal', 'filter_strings_path': None, 'prompt': 'What is the next ${task.item_descriptor} in the following text? Respond only with that single ${task.item_descriptor} and nothing else, including punctuation.\n$string', 'num': '${limit}', 'shuffle': True, 'exclusion_rule_groups': ['default', 'single_word']}, 'response_property': {'name': 'identity', 'python_function': 'identity', 'meta_level_prompt': 'What is the response you gave? Respond only with your response.', 'property_extraction_prompt': None, 'exclusion_rule_groups': ['default'], 'scoring_method': 'exact_match', 'llm_scoring_prompt': None}, 'study_name': 'tiny_test', 'study_dir': '${experiment_folder_location:/${study_name}}', 'exp_dir': '${study_dir}/${sanitize:meta_level_${language_model.model}_${task.name}_task_${n_shot}_shot_${n_shot_seeding}_seed_${prompt.method}_prompt_${response_property.name}_resp_${note}_note}', 'base_dir': None, 'strings_path': 'none', 'filter_strings_path': None, 'note': '', 'seed': 0, 'base_seed': 0, 'limit': 2, 'n_samples': 5, 'reset': False, 'logging': 'INFO', 'print_prompt_and_response': False, 'cache_dir': '${exp_dir}/cache', 'prompt_history_dir': '${exp_dir}/prompt_history', 'n_shot': 0, 'n_shot_seeding': True, 'organization': 'DEFAULT_ORG', 'anthropic_tag': 'ANTHROPIC_API_KEY', 'openai_tag': 'OPENAI_API_KEY', 'anthropic_num_threads': 12, 'openai_fraction_rate_limit': 0.9}
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,941][__main__][INFO] - Using experiment directory /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,941][__main__][INFO] - Using model llama-3-8b-instruct
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,941][__main__][INFO] - Using method meta_level/minimal
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,971][__main__][INFO] - File /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note/raw_data0.csv does not exist. Creating...
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,971][__main__][INFO] - No strings file provided. Using the base data as the strings file.
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,972][__main__][WARNING] - No base data directory provided. Trying to find one in /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,985][evals.generate_few_shot][INFO] - Generating few-shot strings for /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note/data0.csv with 0 few-shot strings
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,985][evals.generate_few_shot][INFO] - Using random seed 0
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note/data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [llama-3-8b-instruct||object_level/minimal|/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note|3|animals]:
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none]   Compliance: 100.00%
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [llama-3-8b-instruct||object_level/minimal|/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note|3|animals]:
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none]   Excluded 0 non-compliant responses, leaving 3 rows
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,004][evals.generate_few_shot][INFO] - Loaded 3 rows from /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note/data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,016][evals.analysis.compliance_checks][INFO] - Excluded non-compliant responses, leaving 3 rows down from 3
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,017][evals.generate_few_shot][INFO] - No strings provided, using the base data as the strings file
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Added /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction to sys.path
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Current git hash: 49ea3810567cc75b539380137a7bd9cde628b807
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] {'prompt': {'method': 'meta_level/minimal', 'base_prompt': 'object_level/minimal', 'messages': [{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'Previously, you were asked the following question: "${task.prompt}" ${response_property.meta_level_prompt}\n'}]}, 'language_model': {'model': 'llama-3-8b-instruct_finetuned_', 'cais_path': '/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned__merged', 'temperature': 0.0, 'top_p': 1.0, 'max_tokens': None, 'num_candidates_per_completion': 1, 'insufficient_valids_behaviour': 'error', 'logprobs': 0}, 'task': {'name': 'animals', 'set': 'val', 'dataset_path': 'evals/datasets/${task.set}_${task.name}.jsonl', 'item_descriptor': 'animal', 'filter_strings_path': None, 'prompt': 'What is the next ${task.item_descriptor} in the following text? Respond only with that single ${task.item_descriptor} and nothing else, including punctuation.\n$string', 'num': '${limit}', 'shuffle': True, 'exclusion_rule_groups': ['default', 'single_word']}, 'response_property': {'name': 'identity', 'python_function': 'identity', 'meta_level_prompt': 'What is the response you gave? Respond only with your response.', 'property_extraction_prompt': None, 'exclusion_rule_groups': ['default'], 'scoring_method': 'exact_match', 'llm_scoring_prompt': None}, 'study_name': 'tiny_test', 'study_dir': '${experiment_folder_location:/${study_name}}', 'exp_dir': '${study_dir}/${sanitize:meta_level_${language_model.model}_${task.name}_task_${n_shot}_shot_${n_shot_seeding}_seed_${prompt.method}_prompt_${response_property.name}_resp_${note}_note}', 'base_dir': None, 'strings_path': 'none', 'filter_strings_path': None, 'note': '', 'seed': 0, 'base_seed': 0, 'limit': 2, 'n_samples': 5, 'reset': False, 'logging': 'INFO', 'print_prompt_and_response': False, 'cache_dir': '${exp_dir}/cache', 'prompt_history_dir': '${exp_dir}/prompt_history', 'n_shot': 0, 'n_shot_seeding': True, 'organization': 'DEFAULT_ORG', 'anthropic_tag': 'ANTHROPIC_API_KEY', 'openai_tag': 'OPENAI_API_KEY', 'anthropic_num_threads': 12, 'openai_fraction_rate_limit': 0.9}
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,957][__main__][INFO] - Using experiment directory /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_finetuned__animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,957][__main__][INFO] - Using model llama-3-8b-instruct_finetuned_
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,957][__main__][INFO] - Using method meta_level/minimal
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,988][__main__][INFO] - File /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_finetuned__animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note/raw_data0.csv does not exist. Creating...
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,988][__main__][INFO] - No strings file provided. Using the base data as the strings file.
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:26,988][__main__][WARNING] - No base data directory provided. Trying to find one in /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,007][evals.generate_few_shot][INFO] - Generating few-shot strings for /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note/data0.csv with 0 few-shot strings
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,007][evals.generate_few_shot][INFO] - Using random seed 0
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note/data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [llama-3-8b-instruct_finetuned_||object_level/minimal|/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note|3|animals]:
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none]   Compliance: 100.00%
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [llama-3-8b-instruct_finetuned_||object_level/minimal|/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note|3|animals]:
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none]   Excluded 0 non-compliant responses, leaving 3 rows
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,026][evals.generate_few_shot][INFO] - Loaded 3 rows from /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note/data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,032][evals.analysis.compliance_checks][INFO] - Excluded non-compliant responses, leaving 3 rows down from 3
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,033][evals.generate_few_shot][INFO] - No strings provided, using the base data as the strings file
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,033][evals.generate_few_shot][WARNING] - Found 3 shared strings between the base data 3 and the strings file (3).
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Added /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction to sys.path
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Current git hash: 49ea3810567cc75b539380137a7bd9cde628b807
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] {'language_model': {'model': 'gpt-3.5-turbo-0125', 'temperature': 0.0, 'top_p': 1.0, 'max_tokens': None, 'num_candidates_per_completion': 1, 'insufficient_valids_behaviour': 'error', 'logprobs': 0}, 'prompt': {'method': 'property-extraction-minimal', 'messages': [{'role': 'system', 'content': ''}, {'role': 'user', 'content': '${response_property.property_extraction_prompt}\n'}]}, 'response_property': {'name': 'identity', 'python_function': 'identity', 'meta_level_prompt': 'What is the response you gave? Respond only with your response.', 'property_extraction_prompt': None, 'exclusion_rule_groups': ['default'], 'scoring_method': 'exact_match', 'llm_scoring_prompt': None}, 'dir': '/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note', 'exp_dir': '${dir}/property_extraction_${response_property.name}_${language_model.model}', 'logging': 'INFO', 'print_prompt_and_response': False, 'cache_dir': '${exp_dir}/cache', 'prompt_history_dir': '${exp_dir}/prompt_history', 'organization': 'DEFAULT_ORG', 'anthropic_tag': 'ANTHROPIC_API_KEY', 'openai_tag': 'OPENAI_API_KEY', 'anthropic_num_threads': 12, 'openai_fraction_rate_limit': 0.9}
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,933][__main__][INFO] - Using experiment directory /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note/property_extraction_identity_gpt-3.5-turbo-0125
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,933][__main__][INFO] - Using model gpt-3.5-turbo-0125
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,933][__main__][INFO] - Using property identity
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,934][__main__][INFO] - Running python function identity
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,934][__main__][INFO] - Applying python function identity to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note/data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,939][__main__][INFO] - Applied python function identity to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note/data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Response property identity not in object level dataframe. Running property extraction with `python -m evals.run_property_extraction response_property=identity dir=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note`.
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loaded response property identity from object level dataframe.
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,590][evals.generate_few_shot][INFO] - Saved 15 strings with few-shot completions to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_finetuned__animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note/raw_data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,590][__main__][INFO] - Generated data0.csv at /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_finetuned__animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note/raw_data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:27,019][evals.generate_few_shot][WARNING] - Found 3 shared strings between the base data 3 and the strings file (3).
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Added /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction to sys.path
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Current git hash: 49ea3810567cc75b539380137a7bd9cde628b807
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] {'language_model': {'model': 'gpt-3.5-turbo-0125', 'temperature': 0.0, 'top_p': 1.0, 'max_tokens': None, 'num_candidates_per_completion': 1, 'insufficient_valids_behaviour': 'error', 'logprobs': 0}, 'prompt': {'method': 'property-extraction-minimal', 'messages': [{'role': 'system', 'content': ''}, {'role': 'user', 'content': '${response_property.property_extraction_prompt}\n'}]}, 'response_property': {'name': 'identity', 'python_function': 'identity', 'meta_level_prompt': 'What is the response you gave? Respond only with your response.', 'property_extraction_prompt': None, 'exclusion_rule_groups': ['default'], 'scoring_method': 'exact_match', 'llm_scoring_prompt': None}, 'dir': '/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note', 'exp_dir': '${dir}/property_extraction_${response_property.name}_${language_model.model}', 'logging': 'INFO', 'print_prompt_and_response': False, 'cache_dir': '${exp_dir}/cache', 'prompt_history_dir': '${exp_dir}/prompt_history', 'organization': 'DEFAULT_ORG', 'anthropic_tag': 'ANTHROPIC_API_KEY', 'openai_tag': 'OPENAI_API_KEY', 'anthropic_num_threads': 12, 'openai_fraction_rate_limit': 0.9}
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,921][__main__][INFO] - Using experiment directory /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note/property_extraction_identity_gpt-3.5-turbo-0125
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,921][__main__][INFO] - Using model gpt-3.5-turbo-0125
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,921][__main__][INFO] - Using property identity
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,921][__main__][INFO] - Running python function identity
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,921][__main__][INFO] - Applying python function identity to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note/data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:32,928][__main__][INFO] - Applied python function identity to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note/data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Response property identity not in object level dataframe. Running property extraction with `python -m evals.run_property_extraction response_property=identity dir=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note`.
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loaded response property identity from object level dataframe.
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,569][evals.generate_few_shot][INFO] - Saved 15 strings with few-shot completions to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note/raw_data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,569][__main__][INFO] - Generated data0.csv at /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note/raw_data0.csv
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Setting n_samples to 1 since using llama-3-8b-instruct
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,571][__main__][INFO] - Processing 2 rows
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,572][evals.apis.inference.huggingface][INFO] - Loading model weights for llama-3-8b-instruct
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,933][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] 
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:01,  2.20it/s]
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:49:17,065][__main__][INFO] - Completed row 0	Running cost: $0.000
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:49:18,048][__main__][INFO] - Completed row 1	Running cost: $0.000
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:49:19,030][__main__][INFO] - Completed row 2	Running cost: $0.000
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:49:19,030][__main__][INFO] - Processed 3 rows. 3 were complete.
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:49:19,035][__main__][INFO] - All rows complete!
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:49:19,036][__main__][INFO] - n_samples is 1, so not collating mode of n.
[python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3] /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_finetuned__object_level_minimal_prompt_animals_val_task__note
Successfully executed: python -m evals.run_object_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals task.set=val prompt=object_level/minimal limit=3 
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] Added /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction to sys.path
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] Current git hash: 49ea3810567cc75b539380137a7bd9cde628b807
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] {'prompt': {'method': 'object_level/minimal', 'messages': [{'role': 'system', 'content': ''}, {'role': 'user', 'content': '${task.prompt}\n'}]}, 'language_model': {'model': 'llama-3-8b-instruct', 'cais_path': 'meta-llama/Meta-Llama-3-8B-Instruct', 'temperature': 0.0, 'top_p': 1.0, 'max_tokens': None, 'num_candidates_per_completion': 1, 'insufficient_valids_behaviour': 'error', 'logprobs': 0}, 'task': {'name': 'animals', 'set': 'val', 'dataset_path': 'evals/datasets/${task.set}_${task.name}.jsonl', 'item_descriptor': 'animal', 'filter_strings_path': None, 'prompt': 'What is the next ${task.item_descriptor} in the following text? Respond only with that single ${task.item_descriptor} and nothing else, including punctuation.\n$string', 'num': '${limit}', 'shuffle': True, 'exclusion_rule_groups': ['default', 'single_word']}, 'study_name': 'tiny_test', 'study_dir': '${experiment_folder_location:/${study_name}}', 'exp_dir': '${study_dir}/${sanitize:object_level_${language_model.model}_${prompt.method}_prompt_${task.name}_${task.set}_task}_${note}_note', 'note': '', 'seed': 0, 'limit': 3, 'n_samples': 5, 'strings_path': None, 'filter_strings_path': None, 'reset': False, 'logging': 'INFO', 'print_prompt_and_response': False, 'cache_dir': '${exp_dir}/cache', 'prompt_history_dir': '${exp_dir}/prompt_history', 'organization': 'DEFAULT_ORG', 'anthropic_tag': 'ANTHROPIC_API_KEY', 'openai_tag': 'OPENAI_API_KEY', 'anthropic_num_threads': 12, 'openai_fraction_rate_limit': 0.9}
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:46:21,982][__main__][INFO] - Using experiment directory /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:46:21,982][__main__][INFO] - Using model llama-3-8b-instruct
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:46:21,982][__main__][INFO] - Using method object_level/minimal
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] Setting n_samples to 1 since using llama or claude
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:46:22,011][__main__][INFO] - File /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note/raw_data0.csv exists. Will not recreate.
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:46:22,013][__main__][INFO] - Processing 0 rows
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:46:22,013][__main__][INFO] - Processed 0 rows. 0 were complete.
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:46:22,016][__main__][INFO] - All rows complete!
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] [2025-06-22 04:46:22,016][__main__][INFO] - n_samples is 1, so not collating mode of n.
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3] /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_val_task__note
Successfully executed: python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3 
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250622_044658-hyn61diy/logs[0m
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Successfully executed: accelerate launch --config_file evals/conf/accelerate_config.yaml --mixed_precision bf16 --main_process_port 15894 --num_processes 1 --gradient_accumulation_steps 8 -m evals.apis.finetuning.hf_finetuning --config evals/conf/trl_config.yaml --output_dir /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ --run_name llama-3-8b-instruct_finetuned_ --model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct --dataset_name /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct --per_device_train_batch_size 4 --gradient_accumulation_steps 8 --learning_rate 0.001 --num_train_epochs 5 --use_peft --lora_r=8 --lora_alpha=16
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] 
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 112.28it/s]
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:47:55,628] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:47:56,690] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] Successfully executed: python evals/apis/finetuning/merge_peft_adapter.py --adapter_model_name /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ --base_model_name meta-llama/Meta-Llama-3-8B-Instruct --output_name /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned__merged
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] [2025-06-22 04:48:47,929][__main__][INFO] - Done with model_id: llama-3-8b-instruct_finetuned_
[python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes=] finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_
Successfully executed: python -m evals.run_finetuning study_name=tiny_test/llama-3-8b-instruct train_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/train_dataset.jsonl val_path=/home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/val_dataset.jsonl language_model=llama-3-8b-instruct notes= 
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Setting n_samples to 1 since using llama-3-8b-instruct_finetuned_
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,593][__main__][INFO] - Processing 2 rows
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,593][evals.apis.inference.huggingface][INFO] - Loading model weights for llama-3-8b-instruct_finetuned_
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:34,839][accelerate.utils.modeling][INFO] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] 
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:00<00:02,  1.16it/s]
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:01<00:01,  1.18it/s]
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:02<00:00,  1.42it/s]
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.79it/s]
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:37,133][accelerate.big_modeling][WARNING] - Some parameters are on the meta device because they were offloaded to the cpu.
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Device set to use cuda:0
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:38,263][evals.apis.inference.huggingface][INFO] - Model weights for llama-3-8b-instruct_finetuned_ loaded; self.pipe.device=device(type='cuda', index=0)
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:39,685][__main__][INFO] - Completed row 0	Running cost: $0.000
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:39,686][__main__][INFO] - Loaded cache for row 1
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:39,686][__main__][INFO] - Processed 2 rows. 2 were complete.
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:39,693][__main__][INFO] - All rows complete!
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:39,693][__main__][INFO] - Only one sample, no need to collate mode
[python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_finetuned__animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note
Successfully executed: python -m evals.run_meta_level study_name=tiny_test language_model=finetuned/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_finetuned_ task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none 
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  8.21it/s]
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:35,670][accelerate.big_modeling][WARNING] - Some parameters are on the meta device because they were offloaded to the cpu.
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Device set to use cuda:0
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:36,779][evals.apis.inference.huggingface][INFO] - Model weights for llama-3-8b-instruct loaded; self.pipe.device=device(type='cuda', index=0)
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:46,323][__main__][INFO] - Completed row 0	Running cost: $0.000
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:46,323][__main__][INFO] - Loaded cache for row 1
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:46,323][__main__][INFO] - Processed 2 rows. 2 were complete.
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:46,330][__main__][INFO] - All rows complete!
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] [2025-06-22 04:49:46,330][__main__][INFO] - Only one sample, no need to collate mode
[python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none] /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/meta_level_llama-3-8b-instruct_animals_task_0_shot_True_seed_meta_level_minimal_prompt_identity_resp__note
Successfully executed: python -m evals.run_meta_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals response_property=identity task.set=val prompt=meta_level/minimal limit=2 strings_path=none 
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] Added /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction to sys.path
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] Current git hash: 49ea3810567cc75b539380137a7bd9cde628b807
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] {'prompt': {'method': 'object_level/minimal', 'messages': [{'role': 'system', 'content': ''}, {'role': 'user', 'content': '${task.prompt}\n'}]}, 'language_model': {'model': 'llama-3-8b-instruct', 'cais_path': 'meta-llama/Meta-Llama-3-8B-Instruct', 'temperature': 0.0, 'top_p': 1.0, 'max_tokens': None, 'num_candidates_per_completion': 1, 'insufficient_valids_behaviour': 'error', 'logprobs': 0}, 'task': {'name': 'animals', 'set': 'train', 'dataset_path': 'evals/datasets/${task.set}_${task.name}.jsonl', 'item_descriptor': 'animal', 'filter_strings_path': None, 'prompt': 'What is the next ${task.item_descriptor} in the following text? Respond only with that single ${task.item_descriptor} and nothing else, including punctuation.\n$string', 'num': '${limit}', 'shuffle': True, 'exclusion_rule_groups': ['default', 'single_word']}, 'study_name': 'tiny_test', 'study_dir': '${experiment_folder_location:/${study_name}}', 'exp_dir': '${study_dir}/${sanitize:object_level_${language_model.model}_${prompt.method}_prompt_${task.name}_${task.set}_task}_${note}_note', 'note': '', 'seed': 0, 'limit': 5, 'n_samples': 5, 'strings_path': None, 'filter_strings_path': None, 'reset': False, 'logging': 'INFO', 'print_prompt_and_response': False, 'cache_dir': '${exp_dir}/cache', 'prompt_history_dir': '${exp_dir}/prompt_history', 'organization': 'DEFAULT_ORG', 'anthropic_tag': 'ANTHROPIC_API_KEY', 'openai_tag': 'OPENAI_API_KEY', 'anthropic_num_threads': 12, 'openai_fraction_rate_limit': 0.9}
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] [2025-06-22 04:46:14,524][__main__][INFO] - Using experiment directory /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_train_task__note
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] [2025-06-22 04:46:14,524][__main__][INFO] - Using model llama-3-8b-instruct
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] [2025-06-22 04:46:14,524][__main__][INFO] - Using method object_level/minimal
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] Setting n_samples to 1 since using llama or claude
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] [2025-06-22 04:46:14,552][__main__][INFO] - File /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_train_task__note/raw_data0.csv exists. Will not recreate.
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] [2025-06-22 04:46:14,555][__main__][INFO] - Processing 0 rows
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] [2025-06-22 04:46:14,555][__main__][INFO] - Processed 0 rows. 0 were complete.
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] [2025-06-22 04:46:14,557][__main__][INFO] - All rows complete!
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] [2025-06-22 04:46:14,558][__main__][INFO] - n_samples is 1, so not collating mode of n.
[python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5] /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/object_level_llama-3-8b-instruct_object_level_minimal_prompt_animals_train_task__note
Successfully executed: python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5 
Skipping python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=train prompt=object_level/minimal limit=5  because it is already complete.
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
Skipping python -m evals.run_object_level study_name=tiny_test language_model=llama-3-8b-instruct task=animals task.set=val prompt=object_level/minimal limit=3  because it is already complete.
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
Skipping divergent strings for animals because it is already complete.
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
File already exists at /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/finetuning/tiny_test/llama-3-8b-instruct/llama-3-8b-instruct_animals_identity_minimal.yaml. Not overwriting.
Created 1 finetuning dataset configs. Creating datasets...
Skipping llama-3-8b-instruct because it is already complete.
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
Created 0 finetuning datasets.
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
Finished running all commands.
State file written to /home/paperspace/PublicRepos/25-06-20-introspection_self_prediction/exp/tiny_test/state.json
