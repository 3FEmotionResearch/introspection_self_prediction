{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are different GPT3.5 versions different?\n",
    "Do different versions of GPT3.5 produce different outputs? This notebook will compare the outputs of different versions of GPT3.5 to see if they are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.locations import REPO_DIR, EXP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELA = \"gpt-3.5-turbo-0125\" # newest model as of 04/12/24\n",
    "MODELB = \"gpt-3.5-turbo-0613\" # oldest model as of 04/12/24, will be deprecated in July 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_NAME = \"comparing_gpt35s_across_versions\"\n",
    "TASKS = ['number_triplets', 'wikipedia', 'writing_stories', 'self_referential']\n",
    "LIMIT = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script to generate the completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in TASKS:\n",
    "    for model in [MODELA, MODELB]:\n",
    "        command = f\"cd {REPO_DIR} && python3 {REPO_DIR}/evals/run_object_level.py study_name={STUDY_NAME} task={task} language_model={model} limit={LIMIT} task.set=val\"\n",
    "        subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to extract some response properties we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_PROPERTIES = ['identity', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in (EXP_DIR/STUDY_NAME).iterdir():\n",
    "    if folder.is_dir():\n",
    "        for response_property in RESPONSE_PROPERTIES:\n",
    "            command = f\"cd {REPO_DIR} && python3 {REPO_DIR}/evals/run_property_extraction.py response_property={response_property} dir={folder}\"\n",
    "            subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "How similar are the object level behaviors themselves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.loading_data import load_dfs_with_filter\n",
    "from evals.utils import get_maybe_nested_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = load_dfs_with_filter(EXP_DIR/STUDY_NAME, conditions={}, exclude_noncompliant=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_dataset(dfs, dataset):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.string_cleaning import clean_string\n",
    "\n",
    "\n",
    "for task in TASKS:\n",
    "    dfs_task = filter_by_dataset(dfs, task)\n",
    "    assert len(dfs_task) == 2\n",
    "    tdfs = list(dfs_task.values())\n",
    "    models = [c['language_model']['model'] for c in dfs_task.keys()]\n",
    "    joint_df = tdfs[0].merge(tdfs[1], on='string', suffixes=('_'+models[0], '_'+models[1]))\n",
    "    for response_property in RESPONSE_PROPERTIES:\n",
    "        joint_df[f\"{response_property}_match\"] = joint_df[f\"{response_property}_{models[0]}\"].apply(clean_string) == joint_df[f\"{response_property}_{models[1]}\"].apply(clean_string)\n",
    "    for response_property in RESPONSE_PROPERTIES:\n",
    "        print(f\"Property: {response_property}\")\n",
    "        print(f\"Task: {task}\")\n",
    "        print(f\"% match:\")\n",
    "        display(joint_df[f\"{response_property}_match\"].value_counts(normalize=True))\n",
    "    print(f\"Task: {task}\")\n",
    "    print(f\"% match:\")\n",
    "    cols = ['string'] + [f\"{response_property}_{model}\" for response_property in RESPONSE_PROPERTIES for model in models]\n",
    "    display(joint_df[~joint_df['identity_match']][cols].sample(min(10, len(joint_df[~joint_df['identity_match']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
