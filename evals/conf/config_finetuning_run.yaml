hydra:
  run:
    dir: ${exp_dir}/logs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${exp_dir}/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO

study_dir: ${experiment_folder_location:/finetuning/${study_name}} # the folder of the study which contains multiple experiments
exp_dir: ${sanitize:${study_dir}/${language_model.model}_${epochs}_epochs}

study_name: ??? # the name of the study

defaults:
  - language_model: gpt-3.5-turbo

epochs: ~ # how many epochs to train for Use `~` to let OpenAI figure it out.
learning_rate: ~ # the learning rate. OpenAI default is the learning rate multiplier of 2. Use `~` to let OpenAI figure it out.
batch_size: ~ # the batch size. OpenAI default is 3. Use `~` to let OpenAI figure it out.
seed: 0 # the seed for the run.
lora_rank: null

notes: ??? # notes about the run

organization: OWAIN_ORG
openai_tag: OPENAI_API_KEY

use_wandb: true
ask_to_validate_training: false
