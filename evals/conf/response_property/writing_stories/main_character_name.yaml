name: writing_stories/main_character_name
python_function: ~ # ~ to use LLM extraction. Function has to be in evals/response_property.py
meta_level_prompt: "What is the name of the main character in the story that you gave? Respond only with that name. If the character was unnamed, respond 'unnamed'. If there was no main character, respond 'none'." # prompt to use for the meta level. This is combined (and read in by) the prompt specified in `prompt`.
property_extraction_prompt: "What is the name of the main character in the following story? Respond only with that name. If the character was unnamed, respond 'unnamed'. If there was no main character, respond 'none'. The story is: '$response'." # prompt to use for the extraction of the property with an LLM. $response will be replaced with the response. Other variables corresponding to columns in the dataframe can be used as well (eg. $target).
# Exclusion rules. See `evals/analysis/compliance.py` for a list of rules.
exclusion_rule_groups: [default] # list of exclusion rules to apply to the responses.

# How are object and meta-level responses to be scored?
scoring_method: "exact_match" # exact_match or LLM.
# If LLM, what is the scoring prompt? Include $object_level_response and $meta_level_response.
llm_scoring_prompt: ~ # prompt to use for scoring with LLM. $object_level_response and $meta_level_response will be replaced with the responses.
