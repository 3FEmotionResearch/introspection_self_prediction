{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating random strings to test for self-consistency\n",
    "\n",
    "Input\n",
    "- .csv from `evals/run_dataset_generation.py` with the base level completions of strings\n",
    "\n",
    "Output\n",
    "- .csv with random strings that are hard to predict from external means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the experiment with the base completions we want to use?\n",
    "EXP = \"default_dir\" # ðŸ”µ\n",
    "FILENAME = \"data0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compliance_checks import check_compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the git command to get the repository root directory\n",
    "REPO_DIR = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"]).decode().strip()\n",
    "\n",
    "print(\"Repository directory:\", REPO_DIR)\n",
    "sys.path.append(REPO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "EXPDIR = Path(REPO_DIR) / \"exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv(EXPDIR / EXP / FILENAME)\n",
    "print(f\"Loaded {len(df)} rows from {EXPDIR / EXP / FILENAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run compliance checks. See `evals/run_dataset_generation.py` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['compliance'] = df['response'].apply(check_compliance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Compliance: {(df['compliance'] == True).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most common non-compliant reasons:\")\n",
    "df[df['compliance'] != True]['compliance'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample some non-compliant responses\n",
    "try:\n",
    "    display(df[df['compliance'] != True].sample(3))\n",
    "except ValueError:\n",
    "    print(\"Not enough non-compliant responses to sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude non-compliant responses\n",
    "df = df[df['compliance'] == True]\n",
    "print(f\"Excluded non-compliant responses, leaving {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to select strings that are hard to predict from external means. Here, we choose those for which the two top first tokens have similar probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add log_prob delta column\n",
    "df[\"logprobs\"] = df[\"logprobs\"].apply(lambda x: eval(x)[0]) # can only be run once\n",
    "# now we have the logprobs for the first token\n",
    "df[\"logprob_delta\"] = df[\"logprobs\"].apply(lambda x: list(x.values())[0] - list(x.values())[1])\n",
    "# Sort by column: 'logprob_delta' (ascending)\n",
    "df = df.sort_values([\"logprob_delta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same with the relative difference\n",
    "df[\"logprob_delta_rel\"] = df[\"logprobs\"].apply(\n",
    "    lambda x: (list(x.values())[0] - list(x.values())[1]) / list(x.values())[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(\n",
    "    x=\"logprob_delta\",\n",
    "    y=\"logprob_delta_rel\",\n",
    "    kind=\"scatter\",\n",
    "    title=\"Relative difference in logprob vs absolute difference in logprob\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column=\"logprob_delta\", bins=100)\n",
    "plt.title(\"Histogram of logprob deltas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"top_logprob\"] = df[\"logprobs\"].apply(lambda x: list(x.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top logprob:\", df[\"top_logprob\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column=\"top_logprob\", bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=\"top_logprob\", y=\"logprob_delta\", kind=\"scatter\")\n",
    "plt.title(\"Top logprob vs logprob delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull the _n_ strings with the closest top 2 probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OUT_STRINGS = 100  # how many strings do we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = df.head(N_OUT_STRINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.hist(column=\"logprob_delta\", bins=50)\n",
    "plt.title(\"Histogram of logprob deltas in the final set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(out_df[\"string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the output strings\n",
    "out_df[\"string\"].to_csv(EXPDIR / EXP / \"out_strings.csv\", index=False)\n",
    "print(f\"Saved {len(out_df)} strings to {EXPDIR / EXP / 'out_strings.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
