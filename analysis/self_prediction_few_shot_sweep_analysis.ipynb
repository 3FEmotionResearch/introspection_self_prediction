{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses for Self Prediction Experiments across different levels of few shot _n_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_FOLDER = \"debug_finetuning_run_tests\" # ðŸ”µ within exp/\n",
    "CONDITIONS = { \n",
    "    # see `analysis/loading_data.py` for details\n",
    "    # (\"language_model\",\"model\"): [\"gpt-4-1106-preview\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo\", \"claude-2.1\"],\n",
    "    # (\"language_model\",\"model\"): [\"davinci-002\"],\n",
    "    (\"dataset\", \"topic\"): [\"english_words\"],\n",
    "    # (\"dataset\",\"n_shot\"): [100, None]\n",
    "    # (\"dataset\",\"n_shot\"): [20, None],\n",
    "    # (\"dataset\",\"n_shot_seeding\"): [\"other_model\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.analysis_helpers import merge_base_and_self_pred_dfs, create_df_from_configs, fill_df_with_function, get_pretty_name, filter_configs_by_conditions, pretty_print_config\n",
    "from evals.analysis.loading_data import load_dfs_with_filter, load_base_df_from_config, get_hydra_config, load_single_df, get_data_path\n",
    "from evals.utils import get_maybe_nested_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\")\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seaborn to shut up\n",
    "import warnings\n",
    "# Ignore the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.generate_few_shot import REPO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "EXPDIR = Path(REPO_DIR) / \"exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with configs as keys\n",
    "dfs = load_dfs_with_filter(EXPDIR / STUDY_FOLDER, CONDITIONS, exclude_noncompliant=False)\n",
    "print(f\"Loaded {len(dfs)} dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might not have the base dirs in the same folder (such as when using other task seeding), so we pull them from the configs we already have\n",
    "c = 0\n",
    "for config, df in list(dfs.items()):\n",
    "    base_dir = get_maybe_nested_from_dict(config, \"base_dir\")\n",
    "    if base_dir:\n",
    "        base_dir = Path(REPO_DIR)/base_dir\n",
    "        base_config = get_hydra_config(Path(base_dir))\n",
    "        if base_config not in dfs:\n",
    "            base_df = load_single_df(get_data_path(base_dir))\n",
    "            dfs[base_config] = base_df\n",
    "            print(f\"Added base config {get_pretty_name(base_config)}\")\n",
    "            c += 1\n",
    "        else:\n",
    "            print(f\"Base config {get_pretty_name(base_config)} already present\")\n",
    "print(f\"Added {c} base configs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base_config(config):\n",
    "    return config[\"prompt\"][\"method\"].startswith(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dfs = {config: df for config, df in dfs.items() if is_base_config(config)}\n",
    "self_pred_dfs = {config: df for config, df in dfs.items() if not is_base_config(config)}\n",
    "print(f\"Loaded {len(base_dfs)} base and {len(self_pred_dfs)} self-prediction dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will merge on the basis which model it is\n",
    "# merge each df with the base df\n",
    "ON = [\"language_model\", \"model\"]\n",
    "merged_self_pred_dfs = {}\n",
    "for base_config, base_df in base_dfs.items():\n",
    "    on_val = get_maybe_nested_from_dict(base_config, ON)\n",
    "    for self_pred_config, self_pred_df in self_pred_dfs.items():\n",
    "        if get_maybe_nested_from_dict(self_pred_config, ON) == on_val:\n",
    "            print(f\"merging with base config:\")\n",
    "            self_pred_config = self_pred_config.copy()\n",
    "            self_pred_config[\"prediction_target\"] = \"self\"  # the model is scored against it's own behavior\n",
    "            merged_self_pred_dfs[self_pred_config] = merge_base_and_self_pred_dfs(\n",
    "                b_df=base_df.copy(),\n",
    "                s_df=self_pred_df.copy(),\n",
    "                string_modifier=get_maybe_nested_from_dict(self_pred_config, (\"dataset\", \"string_modifier\")),\n",
    "                response_property=get_maybe_nested_from_dict(self_pred_config, (\"dataset\", \"response_property\")),\n",
    "            )\n",
    "print(f\"Merged {len(merged_self_pred_dfs)} self-prediction dataframes with base dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_model can be self predicting or other predicting\n",
    "# merge each df with the base df according to base dir\n",
    "other_model_merged_self_pred_dfs = {}\n",
    "for base_config, base_df in base_dfs.items():\n",
    "    base_dir = get_maybe_nested_from_dict(base_config, \"exp_dir\")\n",
    "    base_dir = Path(base_dir).name\n",
    "    for self_pred_config, self_pred_df in self_pred_dfs.items():\n",
    "        if get_maybe_nested_from_dict(self_pred_config, (\"dataset\", \"n_shot_seeding\")) == \"other_model\":\n",
    "            # add flag to config\n",
    "            self_pred_config[\"prediction_target\"] = \"other_model\"  # the model is scored against the few shot seed model\n",
    "            self_pred_base_dir = Path(get_maybe_nested_from_dict(self_pred_config, \"base_dir\")).name\n",
    "            if self_pred_base_dir == base_dir:\n",
    "                print(f\"Merging {get_pretty_name(self_pred_config)} with {get_pretty_name(base_config)}\")\n",
    "                other_model_merged_self_pred_dfs[self_pred_config] = merge_base_and_self_pred_dfs(\n",
    "                    b_df=base_df.copy(),\n",
    "                    s_df=self_pred_df.copy(),\n",
    "                    string_modifier=get_maybe_nested_from_dict(self_pred_config, (\"dataset\", \"string_modifier\")),\n",
    "                    response_property=get_maybe_nested_from_dict(self_pred_config, (\"dataset\", \"response_property\")),\n",
    "                )\n",
    "print(\n",
    "    f\"Merged {len(other_model_merged_self_pred_dfs)} self-prediction dataframes with other_model with base dataframes to be scored against the other model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_self_pred_dfs = {**merged_self_pred_dfs, **other_model_merged_self_pred_dfs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "Create results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results dataframe\n",
    "results = create_df_from_configs(merged_self_pred_dfs.keys())\n",
    "results.sort_values(by=[\"language_model_model\", \"dataset_n_shot_seeding\", \"dataset_n_shot\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a grouping column\n",
    "def grouping(config):\n",
    "    model = get_maybe_nested_from_dict(config, (\"language_model\", \"model\"))\n",
    "    n_shot_seeding = get_maybe_nested_from_dict(config, (\"dataset\", \"n_shot_seeding\"))\n",
    "    topic = get_maybe_nested_from_dict(config, (\"dataset\", \"topic\"))\n",
    "    prediction_target = get_maybe_nested_from_dict(config, \"prediction_target\")\n",
    "    prompt = get_maybe_nested_from_dict(config, (\"prompt\", \"method\"))\n",
    "    return f\"{model} ({n_shot_seeding} seeded) on {topic}\\nscored against {prediction_target} using {prompt}\"\n",
    "\n",
    "results[\"grouping\"] = results[\"config\"].apply(grouping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would like to know how likely the model is to give the correct answer. However, the Chat API does not allow us to get the likelihood of a given response, so we use the likelihood of the first token as a proxy. If the correct response is not in the list of top logprobs, we assume the likelihood is flat over all other tokens, which our token is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the probability of the correct first logprob\n",
    "VOCAB_SIZE = 50257 # gpt-3.5-turboâ€”using the same vocab size for all models\n",
    "\n",
    "def first_log_prob_likelihood(row):\n",
    "    target_token = row['first_token_base']\n",
    "    logprobs = row['first_logprobs_self']\n",
    "    if logprobs is None:\n",
    "        return None\n",
    "    if isinstance(logprobs, str):\n",
    "        logprobs = eval(logprobs)\n",
    "    try:\n",
    "        if target_token in logprobs:\n",
    "            return logprobs[target_token]\n",
    "        else:\n",
    "            # the log prob is not in the top n, so we calculate the flat probability of the outside of the top n\n",
    "            top_n_mass = np.sum([v for k,v in logprobs.items()])\n",
    "            outside_top_n_mass = 1 - top_n_mass\n",
    "            return np.log(outside_top_n_mass / (VOCAB_SIZE - len(logprobs)))\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "for config in merged_self_pred_dfs:\n",
    "    df = merged_self_pred_dfs[config]\n",
    "    df['first_logprob_likelihood'] = df.apply(first_log_prob_likelihood, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many strings are correctly produced by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_POSSIBLE_ITEMS = len(words.words()) # what is the number of possible items in the string? ðŸ”µ\n",
    "N_POSSIBLE_ITEMS = 1000 # ðŸ”µ\n",
    "# N_POSSIBLE_ITEMS = 2 # ðŸ”µ\n",
    "print(f\"Number of possible items in the string: {N_POSSIBLE_ITEMS},\\nwhich gives us a probability of {1/N_POSSIBLE_ITEMS:.6%} for a random guess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_noncompliant(df):\n",
    "    df = df.copy()\n",
    "    df = df[(df['compliance_self'] == True) & (df['compliance_base'] == True)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(df):\n",
    "    \"\"\"Calculate the accuracy of the model\"\"\"\n",
    "    df = exclude_noncompliant(df)\n",
    "    return (df['response_self'] == df['response_base']).mean()\n",
    "\n",
    "def calc_accuracy_with_excluded(df):\n",
    "    \"\"\"What is the accuracy if we count non-compliance as wrong answers?\"\"\"\n",
    "    df['correct'] = df['response_self'] == df['response_base']\n",
    "    df['correct'] = df['correct'] & (df['compliance_self'] == True)\n",
    "    return df['correct'].mean()\n",
    "\n",
    "def calc_t(df):\n",
    "    \"\"\"Calculate the t-statistic of the model\"\"\"\n",
    "    df = exclude_noncompliant(df)\n",
    "    t, p = stats.ttest_1samp(df['response_self'] == df['response_base'], 1/N_POSSIBLE_ITEMS)\n",
    "    return t, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(df, num_bootstraps=1000, ci=95):\n",
    "    df = df[(df[\"compliance_self\"] == True) & (df[\"compliance_base\"] == True)]\n",
    "\n",
    "    bootstrap_accuracies = []\n",
    "\n",
    "    # Resampling the data frame with replacement and calculating accuracies\n",
    "    for _ in range(num_bootstraps):\n",
    "        resampled_df = df.sample(n=len(df), replace=True)\n",
    "        accuracy = calc_accuracy(resampled_df)\n",
    "        bootstrap_accuracies.append(accuracy)\n",
    "\n",
    "    # Calculating the lower and upper percentiles\n",
    "    lower_percentile = (100 - ci) / 2\n",
    "    upper_percentile = 100 - lower_percentile\n",
    "    ci_lower = np.percentile(bootstrap_accuracies, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_accuracies, upper_percentile)\n",
    "\n",
    "    return ci_lower, ci_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_base_df_for_config_from_dict(config):\n",
    "    assert \"base_dir\" in config, \"No base_dir found in configâ€”are you passing a self prediction config?\"\n",
    "    base_dir = config[\"base_dir\"]\n",
    "    # go thru base_dfs and find the one with the same base_dir\n",
    "    found = []\n",
    "    for base_config, base_df in base_dfs.items():\n",
    "        if base_config[\"exp_dir\"] == base_dir:\n",
    "            found.append(base_df)\n",
    "    if len(found) > 1:\n",
    "        raise ValueError(f\"More than one base dataframe found for {config}\")\n",
    "    elif len(found) == 1:\n",
    "        return found[0]\n",
    "    else:\n",
    "        raise ValueError(f\"No base dataframe found for {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_accuracy_under_mode(df, config):\n",
    "    \"\"\"What would be the accuracy if the model always picked the most common response in the base responses?\"\"\"\n",
    "    df = exclude_noncompliant(df)\n",
    "    # load base df\n",
    "    # base_df = load_base_df_from_config(config)\n",
    "    base_df = pull_base_df_for_config_from_dict(config)\n",
    "    mode = base_df['response'].mode()[0]\n",
    "    accuracy_under_mode = (df['response_base'] == mode).mean()\n",
    "    return accuracy_under_mode\n",
    "\n",
    "def baseline_accuracy_under_distribution(df, config, iters = 100):\n",
    "    \"\"\"What would be the accuracy if the model always picked a response from the base responses according to the distribution?\"\"\"\n",
    "    df = exclude_noncompliant(df)\n",
    "    # load base df\n",
    "    # base_df = load_base_df_from_config(config)\n",
    "    base_df = pull_base_df_for_config_from_dict(config)\n",
    "    accuracies = []\n",
    "    for i in range(iters):\n",
    "        # randomly sample from the base responses\n",
    "        sample = np.random.choice(base_df['response'], len(df), replace=True)\n",
    "        accuracy = (df['response_base'] == sample).mean()\n",
    "        accuracies.append(accuracy)\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_of_correct_first_token(df):\n",
    "    df = exclude_noncompliant(df)\n",
    "    return df['first_logprob_likelihood'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the results dataframe with the accuracy and t-statistic\n",
    "fill_df_with_function(merged_self_pred_dfs, calc_accuracy, \"accuracy\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, calc_accuracy_with_excluded, \"accuracy_with_noncompliant\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, calc_t, \"t_statistic\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, bootstrap_ci, \"bootstrap_ci\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, baseline_accuracy_under_mode, \"mode_baseline_accuracy\", results, pass_config=True)\n",
    "fill_df_with_function(merged_self_pred_dfs, baseline_accuracy_under_distribution, \"distribution_baseline_accuracy\", results, pass_config=True)\n",
    "fill_df_with_function(merged_self_pred_dfs, lambda df: (df['compliance_self'] == True).\n",
    "mean(), \"compliance\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, likelihood_of_correct_first_token, \"likelihood_of_correct_first_token\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))  # Set the figsize to (12, 6) for a larger figure\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"accuracy\", hue=\"grouping\")\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"mode_baseline_accuracy\", hue=\"grouping\", linestyles='dotted', markers='', alpha=0.33)\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"distribution_baseline_accuracy\", hue=\"grouping\", linestyles='dashdot', markers='', alpha=0.33)\n",
    "\n",
    "plt.axhline(y=1/N_POSSIBLE_ITEMS, linestyle='dotted', color='grey', label=\"Chance\")\n",
    "plt.title(f\"Self-prediction accuracy per number of examples shown\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# Scale y labels by 100 to get percent\n",
    "plt.yticks(plt.yticks()[0], [f\"{int(tick*100)}%\" for tick in plt.yticks()[0]])\n",
    "\n",
    "# add a legend for the baselines\n",
    "plt.text(1.06, 0, \"...    mode baseline\\n._._  sampling baseline\", transform=plt.gca().transAxes, fontsize=10, color='grey')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))  # Set the figsize to (12, 6) for a larger figure\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"accuracy_with_noncompliant\", hue=\"grouping\")\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"mode_baseline_accuracy\", hue=\"grouping\", linestyles='dotted', markers='', alpha=0.33)\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"distribution_baseline_accuracy\", hue=\"grouping\", linestyles='dashdot', markers='', alpha=0.33)\n",
    "\n",
    "plt.axhline(y=1/N_POSSIBLE_ITEMS, linestyle='dotted', color='grey', label=\"Chance\")\n",
    "plt.title(f\"Self-prediction accuracy (non-compliant responses included) per number of examples shown\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# Scale y labels by 100 to get percent\n",
    "plt.yticks(plt.yticks()[0], [f\"{int(tick*100)}%\" for tick in plt.yticks()[0]])\n",
    "\n",
    "# add a legend for the baselines\n",
    "plt.text(1.06, 0, \"...    mode baseline\\n._._  sampling baseline\", transform=plt.gca().transAxes, fontsize=10, color='grey')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))  # Set the figsize to (12, 6) for a larger figure\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"likelihood_of_correct_first_token\", hue=\"grouping\")\n",
    "plt.title(f\"Mean log probability of the first base token under the self prediction distribution per number of examples shown\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"Mean Log Probability\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"compliance\", hue=\"grouping\")\n",
    "plt.title(f\"Compliance per number of examples shown\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"Compliance in %\")\n",
    "plt.ylim(0, 1.01)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# Scale y labels by 100 to get percent\n",
    "plt.yticks(plt.yticks()[0], [f\"{int(tick*100)}%\" for tick in plt.yticks()[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))  # Set the figsize to (12, 6) for a larger figure\n",
    "for i, (label, group) in enumerate(results.groupby(\"grouping\")):\n",
    "    # Choose color from the palette\n",
    "    color = palette[i]\n",
    "\n",
    "    plt.errorbar(\n",
    "        x=group[\"dataset_n_shot\"],\n",
    "        y=group[\"accuracy\"],\n",
    "        yerr=group[\"bootstrap_ci\"].apply(lambda x: (x[1] - x[0]) / 2),\n",
    "        fmt=\"o\",\n",
    "        capsize=5,\n",
    "        label=label,\n",
    "        color=color,\n",
    "    )\n",
    "    plt.plot(\n",
    "        group[\"dataset_n_shot\"],\n",
    "        group[\"accuracy\"],\n",
    "        marker=\"o\",\n",
    "        # label=label,\n",
    "        color=color,\n",
    "    )\n",
    "plt.axhline(y=1 / N_POSSIBLE_ITEMS, linestyle=\"dotted\", color=\"grey\", label=\"Chance\")\n",
    "plt.title(f\"Self-prediction accuracy as a function of few-shot n\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "# Scale y labels by 100 to get percent\n",
    "plt.yticks(plt.yticks()[0], [f\"{int(tick*100)}%\" for tick in plt.yticks()[0]])\n",
    "plt.xticks(results[\"dataset_n_shot\"])\n",
    "# plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at results\n",
    "results.drop(columns=[\"config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do models follow cheap strategies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much variance is there in the responses? Calculate Shannon entropy over responses\n",
    "def calc_entropy(df, col):\n",
    "    \"\"\"Calculate the entropy of the model\"\"\"\n",
    "    return stats.entropy(df[col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also check if the model is following some cheap strategy\n",
    "fill_df_with_function(merged_self_pred_dfs, lambda df: df['last_word_repeated_self'].mean(), \"last_word_repeated_self\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, lambda df: df['last_char_repeated_self'].mean(), \"last_char_repeated_self\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, lambda df: df['last_word_repeated_base'].mean(), \"last_word_repeated_base\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, lambda df: df['last_char_repeated_base'].mean(), \"last_char_repeated_base\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, lambda df: df['any_word_repeated_self'].mean(), \"any_word_repeated_self\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, lambda df: df['any_word_repeated_base'].mean(), \"any_word_repeated_base\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, lambda df: calc_entropy(df, \"response_self\"), \"entropy_self\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, lambda df: calc_entropy(df, \"response_base\"), \"entropy_base\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"last_word_repeated_self\", hue=\"grouping\")\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"last_word_repeated_base\", hue=\"grouping\", alpha = 0.33)\n",
    "plt.axhline(y=1/N_POSSIBLE_ITEMS, linestyle='dotted', color='grey', label=\"Chance\")\n",
    "plt.title(\"Is the last item repeated?\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"% Last item repeated\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.yticks(plt.yticks()[0], [f\"{int(tick*100)}%\" for tick in plt.yticks()[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"last_char_repeated_self\", hue=\"grouping\")\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"last_char_repeated_base\", hue=\"grouping\", alpha = 0.33)\n",
    "plt.axhline(y=1/N_POSSIBLE_ITEMS, linestyle='dotted', color='grey', label=\"Chance\")\n",
    "plt.title(\"Is the last character repeated?\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"% Last item repeated\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.yticks(plt.yticks()[0], [f\"{int(tick*100)}%\" for tick in plt.yticks()[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"any_word_repeated_self\", hue=\"grouping\")\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"any_word_repeated_base\", hue=\"grouping\", alpha = 0.33)\n",
    "plt.axhline(y=1/N_POSSIBLE_ITEMS, linestyle='dotted', color='grey', label=\"Chance\")\n",
    "plt.title(\"Is the response in the string?\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"% Last item repeated\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.yticks(plt.yticks()[0], [f\"{int(tick*100)}%\" for tick in plt.yticks()[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"entropy_self\", hue=\"grouping\")\n",
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"entropy_base\", hue=\"grouping\", alpha = 0.33)\n",
    "plt.title(\"Shannon Entropy of responses\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"Shannon Entropy of responses\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.ylim(0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed analysis\n",
    "\n",
    "Name conditions, and it will pull the relevant dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_conditions = { \n",
    "    (\"language_model\",\"model\"): [\"gpt-3.5-turbo-0125\"],\n",
    "    # (\"language_model\",\"model\"): [\"claude-2.1\"],\n",
    "    # (\"language_model\",\"model\"): [\"davinci-002\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-4-1106-preview\"],\n",
    "    (\"dataset\",\"n_shot\"): [9], \n",
    "    # (\"prediction_target\"): [\"self\"],\n",
    "    # (\"dataset\",\"n_shot_seeding\"): [True]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_configs = filter_configs_by_conditions(merged_self_pred_dfs.keys(), filter_conditions)\n",
    "print(f\"Got {len(filtered_configs)}, down from {len(merged_self_pred_dfs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in filtered_configs: pretty_print_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dfs\n",
    "filtered_merged_dfs = {config: df for config, df in merged_self_pred_dfs.items() if config in filtered_configs}\n",
    "print(f\"Got {len(filtered_merged_dfs)}, down from {len(merged_self_pred_dfs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    display(detail_df.sample(5))\n",
    "    # display(detail_df[[\"string\", \"response_base\", \"response_self\", \"raw_response_self\",  'few-shot_string', 'few-shot_response']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    # what are the most common base predictions pairs?\n",
    "    display(detail_df[[\"response_base\"]].value_counts(normalize=True).head(10) * 100)\n",
    "\n",
    "    # Filter out non-numeric values\n",
    "    detail_df['response_base_numeric'] = detail_df['response_base'].apply(lambda x: int(x) if x.isnumeric() else None)\n",
    "\n",
    "    detail_df[\"response_base_numeric\"].dropna().hist(range=(0, N_POSSIBLE_ITEMS), bins=N_POSSIBLE_ITEMS, color = \"turquoise\")\n",
    "    plt.title(\"Distribution of base predictions\")\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    display(detail_df[[\"response_self\"]].value_counts(normalize=True).head(10) * 100)\n",
    "\n",
    "    # Filter out non-numeric values\n",
    "    detail_df['response_self_numeric'] = detail_df['response_self'].apply(lambda x: int(x) if x.isnumeric() else None)\n",
    "\n",
    "    detail_df[\"response_self_numeric\"].dropna().hist(range=(0, N_POSSIBLE_ITEMS), bins=N_POSSIBLE_ITEMS, color = \"orange\")\n",
    "    plt.title(\"Distribution of self predictions\")\n",
    "    plt.xlabel(\"Completion\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    # Filter out non-numeric values\n",
    "    detail_df['response_base_numeric'] = detail_df['response_base'].apply(lambda x: int(x) if x.isnumeric() else None)\n",
    "    detail_df['response_self_numeric'] = detail_df['response_self'].apply(lambda x: int(x) if x.isnumeric() else None)\n",
    "\n",
    "    # Drop NA values\n",
    "    base_numeric = detail_df[\"response_base_numeric\"].dropna()\n",
    "    self_numeric = detail_df[\"response_self_numeric\"].dropna()\n",
    "\n",
    "    # Discard outliers\n",
    "    base_numeric = base_numeric[base_numeric.between(base_numeric.quantile(.05), base_numeric.quantile(.95))]\n",
    "    self_numeric = self_numeric[self_numeric.between(self_numeric.quantile(.05), self_numeric.quantile(.95))]\n",
    "\n",
    "    # Plot Kernel Density Estimate\n",
    "    sns.kdeplot(base_numeric, color=\"turquoise\", label=\"Base Predictions\")\n",
    "    sns.kdeplot(self_numeric, color=\"orange\", label=\"Self Predictions\")\n",
    "\n",
    "    plt.xlim(0, N_POSSIBLE_ITEMS)\n",
    "    plt.title(\"Distribution of Base and Self Predictions\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    # lets make a confusion matrix\n",
    "    # filter the df to the top N responses\n",
    "    TOP_N = N_POSSIBLE_ITEMS\n",
    "    top_responses = detail_df[\"response_base\"].value_counts().head(TOP_N).index\n",
    "    detail_top_responses_df = detail_df[\n",
    "        (detail_df[\"response_base\"].isin(top_responses)) & (detail_df[\"response_self\"].isin(top_responses))\n",
    "    ]\n",
    "    sns.heatmap(\n",
    "        pd.crosstab(detail_top_responses_df[\"response_self\"], detail_top_responses_df[\"response_base\"]),\n",
    "        cmap=\"YlGnBu\",\n",
    "        annot=False,\n",
    "    )\n",
    "    plt.title(f\"Confusion matrix of self prediction (top {TOP_N} shown)\")\n",
    "    plt.xlabel(\"Self prediction\")\n",
    "    plt.ylabel(\"Base ground truth\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    # lets make a confusion matrix\n",
    "    sns.heatmap(\n",
    "        pd.crosstab(detail_df[\"response_self\"], detail_df[\"response_base\"]),\n",
    "        cmap=\"YlGnBu\",\n",
    "        annot=False,\n",
    "    )\n",
    "    plt.title(\"Confusion matrix of self prediction (all shown)\")\n",
    "    plt.xlabel(\"Self prediction\")\n",
    "    plt.ylabel(\"Base ground truth\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    print(\"\\nMost common response pairs\")\n",
    "    # what are the most common response pairs?\n",
    "    display(detail_df[[\"response_base\", \"response_self\"]].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the correctness where the last item is repeated/not repeated?\n",
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    _df = detail_df.copy()\n",
    "    pretty_print_config(config=config)\n",
    "    detail_df[\"correct\"] = detail_df[\"response_self\"] == detail_df[\"response_base\"]\n",
    "    detail_df[\"response_in_string_self\"] = detail_df.apply(lambda x: str(x['response_self']) in str(x['string']), axis=1)\n",
    "    detail_df[\"response_in_string_base\"] = detail_df.apply(lambda x: str(x['response_base']) in str(x['string']), axis=1)\n",
    "    # what is the correctness where the last item is repeated/not repeated?\n",
    "    for measure in [\"last_word_repeated_base\",\"response_in_string_base\"]:\n",
    "        display(pd.DataFrame(detail_df.groupby(measure)[\"correct\"].mean() * 100).style.background_gradient(cmap='RdYlGn'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    display(detail_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
