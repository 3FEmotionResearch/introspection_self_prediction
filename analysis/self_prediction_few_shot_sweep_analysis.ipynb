{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyses for Self Prediction Experiments across different levels of few shot _n_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the experiment with the base completions we want to use?\n",
    "STUDY_FOLDER = \"english_words_basic\" # ðŸ”µ within exp/\n",
    "CONDITIONS = { # see `analysis/loading_data.py` for details\n",
    "    # \"language_model\": [\"gpt-3.5-turbo\", \"gpt-4-turbo\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the git command to get the repository root directory\n",
    "REPO_DIR = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"]).decode().strip()\n",
    "\n",
    "print(\"Repository directory:\", REPO_DIR)\n",
    "sys.path.append(REPO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_helpers import merge_base_and_self_pred_dfs, create_df_from_configs, fill_df_with_function\n",
    "from loading_data import load_dfs_with_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\")\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "EXPDIR = Path(REPO_DIR) / \"exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with configs as keys\n",
    "dfs = load_dfs_with_filter(EXPDIR / STUDY_FOLDER, CONDITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base_config(config):\n",
    "    return \"base\" in config[\"prompt\"][\"method\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dfs = {config: df for config, df in dfs.items() if is_base_config(config)}\n",
    "self_pred_dfs = {config: df for config, df in dfs.items() if not is_base_config(config)}\n",
    "print(f\"Loaded {len(base_dfs)} base and {len(self_pred_dfs)} self-prediction dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge each df with the base df\n",
    "ON = \"language_model\"\n",
    "merged_self_pred_dfs = {}\n",
    "for base_config, base_df in base_dfs.items():\n",
    "    on_val = base_config[ON]\n",
    "    for self_pred_config, self_pred_df in self_pred_dfs.items():\n",
    "        if self_pred_config[ON] == on_val:\n",
    "            merged_self_pred_dfs[self_pred_config] = merge_base_and_self_pred_dfs(base_df, self_pred_df)\n",
    "print(f\"Merged {len(merged_self_pred_dfs)} self-prediction dataframes with base dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results dataframe\n",
    "results = create_df_from_configs(merged_self_pred_dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many strings are correctly produced by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POSSIBLE_ITEMS = len(words.words()) # what is the number of possible items in the string?\n",
    "# N_POSSIBLE_ITEMS = 10\n",
    "print(f\"Number of possible items in the string: {N_POSSIBLE_ITEMS},\\nwhich gives us a probability of {1/N_POSSIBLE_ITEMS:.6%} for a random guess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(df):\n",
    "    \"\"\"Calculate the accuracy of the model\"\"\"\n",
    "    return (df['response_self'] == df['response_base']).mean()\n",
    "\n",
    "def calc_t(df):\n",
    "    \"\"\"Calculate the t-statistic of the model\"\"\"\n",
    "    t, p = stats.ttest_1samp(df['response_self'] == df['response_base'], 1/N_POSSIBLE_ITEMS)\n",
    "    return t, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_ci(df, num_bootstraps=1000, ci=95):\n",
    "    bootstrap_accuracies = []\n",
    "\n",
    "    # Resampling the data frame with replacement and calculating accuracies\n",
    "    for _ in range(num_bootstraps):\n",
    "        resampled_df = df.sample(n=len(df), replace=True)\n",
    "        accuracy = calc_accuracy(resampled_df)\n",
    "        bootstrap_accuracies.append(accuracy)\n",
    "\n",
    "    # Calculating the lower and upper percentiles\n",
    "    lower_percentile = (100 - ci) / 2\n",
    "    upper_percentile = 100 - lower_percentile\n",
    "    ci_lower = np.percentile(bootstrap_accuracies, lower_percentile)\n",
    "    ci_upper = np.percentile(bootstrap_accuracies, upper_percentile)\n",
    "\n",
    "    return ci_lower, ci_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the results dataframe with the accuracy and t-statistic\n",
    "fill_df_with_function(merged_self_pred_dfs, calc_accuracy, \"accuracy\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, calc_t, \"t_statistic\", results)\n",
    "fill_df_with_function(merged_self_pred_dfs, bootstrap_ci, \"bootstrap_ci\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"accuracy\", hue=\"language_model\")\n",
    "# plt.errorbar(x=few_shot_ns, y=accuracies, yerr=np.array(cis).T, fmt='o', capsize=5)\n",
    "plt.axhline(y=1/N_POSSIBLE_ITEMS, linestyle='dotted', color='grey', label=\"Chance\")\n",
    "plt.title(f\"Self-prediction accuracy per number of examples shown\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.legend()\n",
    "# scale y labels by 100 to get percent\n",
    "plt.yticks(plt.yticks()[0], [f\"{int(tick*100)}%\" for tick in plt.yticks()[0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(by=\"dataset_n_shot\", ascending=True, inplace=True)\n",
    "for i, (label, group) in enumerate(results.groupby(\"language_model\")):\n",
    "    # Choose color from the palette\n",
    "    color = palette[i]\n",
    "\n",
    "    plt.errorbar(\n",
    "        x=group[\"dataset_n_shot\"],\n",
    "        y=group[\"accuracy\"],\n",
    "        yerr=group[\"bootstrap_ci\"].apply(lambda x: (x[1] - x[0]) / 2),\n",
    "        fmt=\"o\",\n",
    "        capsize=5,\n",
    "        label=label,\n",
    "        color=color,\n",
    "    )\n",
    "    plt.plot(\n",
    "        group[\"dataset_n_shot\"],\n",
    "        group[\"accuracy\"],\n",
    "        marker=\"o\",\n",
    "        # label=label,\n",
    "        color=color,\n",
    "    )\n",
    "plt.axhline(y=1 / N_POSSIBLE_ITEMS, linestyle=\"dotted\", color=\"grey\", label=\"Chance\")\n",
    "plt.title(f\"Self-prediction accuracy as a function of few-shot n\")\n",
    "plt.xlabel(\"Few-shot n\")\n",
    "plt.ylabel(\"Accuracy in %\")\n",
    "plt.legend()\n",
    "# Scale y labels by 100 to get percent\n",
    "plt.yticks(plt.yticks()[0], [f\"{int(tick*100)}%\" for tick in plt.yticks()[0]])\n",
    "plt.xticks(results[\"dataset_n_shot\"])\n",
    "# plt.xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
