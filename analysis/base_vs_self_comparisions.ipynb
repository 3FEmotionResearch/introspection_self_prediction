{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing base experiments against self predictions\n",
    "This notebook compares how well different models do scored against base predictions from itself or other models. This is most useful in checking finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_FOLDERS = [ # ðŸ”µ within exp/\n",
    "    \"number_triplets_bergenia_ft_self_pred\", \n",
    "    \"number_triplets_bergenia_finetuned_small_dataset\",\n",
    "    # \"random_words_bergenia\",\n",
    "    # \"number_triplets_bergenia\",\n",
    "]\n",
    "    \n",
    "CONDITIONS = { \n",
    "    # see `analysis/loading_data.py` for details\n",
    "    # (\"language_model\",\"model\"): [\"gpt-4-1106-preview\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo-1106\", \"gpt-4-0613\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo-1106\"],\n",
    "    # (\"prompt\", \"method\"): [\"base-completion-bergenia\", \"self-prediction-bergenia-nontechnical\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo\", \"claude-2.1\"],\n",
    "    # (\"language_model\",\"model\"): [\"davinci-002\"],\n",
    "    # (\"dataset\", \"topic\"): [\"number_triplets\"],\n",
    "    # (\"dataset\", \"topic\"): [\"english_words\"],\n",
    "    # (\"dataset\",\"n_shot\"): [100, None]\n",
    "    (\"dataset\",\"n_shot\"): [0, None],\n",
    "    # (\"dataset\",\"n_shot_seeding\"): [\"other_model\"]\n",
    "    ('dataset', 'string_modifier'): ['None', None],\n",
    "    ('dataset', 'response_property'): ['None', None],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.analysis_helpers import merge_base_and_self_pred_dfs, create_df_from_configs, fill_df_with_function, get_pretty_name, filter_configs_by_conditions, pretty_print_config, get_pretty_name_w_labels\n",
    "from evals.analysis.loading_data import load_dfs_with_filter, load_base_df_from_config, get_hydra_config, load_single_df, get_data_path\n",
    "from evals.analysis.analysis_functions import *\n",
    "from evals.utils import get_maybe_nested_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\", 64)\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seaborn to shut up\n",
    "import warnings\n",
    "# Ignore the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.generate_few_shot import REPO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "EXPDIR = Path(REPO_DIR) / \"exp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataframes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with configs as keys\n",
    "dfs = {}\n",
    "for STUDY_FOLDER in STUDY_FOLDERS:\n",
    "    _dfs = load_dfs_with_filter(EXPDIR / STUDY_FOLDER, CONDITIONS, exclude_noncompliant=False)\n",
    "    dfs.update(_dfs)\n",
    "    print(f\"Loaded {len(_dfs)} dataframes from {STUDY_FOLDER}\")\n",
    "print(f\"Loaded {len(dfs)} dataframes in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base_config(config):\n",
    "    return config[\"prompt\"][\"method\"].startswith(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dfs = {config: df for config, df in dfs.items() if is_base_config(config)}\n",
    "self_pred_dfs = {config: df for config, df in dfs.items() if not is_base_config(config)}\n",
    "print(f\"Loaded {len(base_dfs)} base and {len(self_pred_dfs)} self-prediction dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have the following datasets:\")\n",
    "datasets = set([get_maybe_nested_from_dict(k, ('dataset', 'topic')) for k in base_dfs.keys()])\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in base_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in self_pred_dfs.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LABELS = {\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnum:8x4lehAb\": \"GPT3.5 fted on GPT3.5\" ,\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnumscram:8x6QzXiQ\": \"GPT3.5 fted on GPT3.5\\n(scrambled)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnum:8xMcmGZM\": \"GPT3.5 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on4onnum:8x8dNwL1\": \"GPT4 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on35onnum:8xq9fNVt\": \"GPT4 fted on GPT3.5\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnums:8zFjiOFt\": \"GPT3.5 fted on GPT3.5 (small dataset)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnums:8zHmk4o8\": \"GPT3.5 fted on GPT4 (small dataset)\",\n",
    "    \"gpt-3.5-turbo-1106\": \"GPT3.5\",\n",
    "    \"gpt-4-0613\": \"GPT4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wo_labels = [l for l in {get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in base_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in self_pred_dfs.keys()}) if l not in MODEL_LABELS]\n",
    "if len(models_wo_labels) > 0: print(\"Models without labels:\") \n",
    "else: print(\"All models have labels\")\n",
    "for m in models_wo_labels:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(config):\n",
    "    label = \"\"\n",
    "    if isinstance(config, str):\n",
    "        config = eval(config)\n",
    "    model = get_maybe_nested_from_dict(config, ('language_model', 'model'))\n",
    "    if model in MODEL_LABELS:\n",
    "        model = MODEL_LABELS[model]\n",
    "    label += model\n",
    "    response_property = get_maybe_nested_from_dict(config, ('dataset', 'response_property'))\n",
    "    if response_property not in [\"None\", None]:\n",
    "        label += f\"\\n predicting {response_property}\"\n",
    "    string_modifier = get_maybe_nested_from_dict(config, ('dataset', 'string_modifier'))\n",
    "    if string_modifier not in [\"None\", None]:\n",
    "        label += f\"\\nw string mod:{string_modifier}\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairwise_tables(measure, base_dfs, self_pred_dfs):\n",
    "    results = pd.DataFrame(columns=[str(config) for config in base_dfs.keys()], index=[str(config) for config in self_pred_dfs.keys()])\n",
    "    for base_config, base_df in base_dfs.items():\n",
    "        for self_config, self_df in self_pred_dfs.items():\n",
    "            joint_df = merge_base_and_self_pred_dfs(\n",
    "                base_df,\n",
    "                self_df,\n",
    "                string_modifier=get_maybe_nested_from_dict(self_config, (\"dataset\", \"string_modifier\")),\n",
    "                response_property=get_maybe_nested_from_dict(self_config, (\"dataset\", \"response_property\")),\n",
    "            )\n",
    "            results.loc[str(self_config), str(base_config)] = measure(joint_df)\n",
    "    results.index = results.index.map(get_label)\n",
    "    results.columns = results.columns.map(get_label)\n",
    "    # sort the columns and the rows\n",
    "    results = results.sort_index(axis=0)\n",
    "    results = results.sort_index(axis=1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_dataset(dfs, dataset):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('dataset', 'topic')) == dataset}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    results = make_pairwise_tables(calc_accuracy_with_excluded, filter_by_dataset(base_dfs, dataset), filter_by_dataset(self_pred_dfs, dataset))\n",
    "    print(f\"Accuracy for {dataset}\")\n",
    "    \n",
    "    sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, fmt=\".0%\")\n",
    "    plt.xlabel(\"Scored against object-level\")\n",
    "    plt.ylabel(\"Meta-level\")\n",
    "    plt.title(f\"Accuracy of meta-level predicting different object-level models on {dataset}\")\n",
    "    plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base vs base change heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    results = make_pairwise_tables(calc_accuracy, filter_by_dataset(base_dfs, dataset), filter_by_dataset(base_dfs, dataset))\n",
    "    print(f\"Overlap between object-level completions for {dataset}\")\n",
    "    \n",
    "    mask = np.triu(np.ones_like(results, dtype=bool), k=1)\n",
    "    sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, fmt=\".0%\", mask=mask)\n",
    "    # plt.xlabel(\"Scored against object-level\")\n",
    "    # plt.ylabel(\"Meta-level\")\n",
    "    plt.title(f\"Overlap between object-level completions for {dataset}\")\n",
    "    plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: stats.entropy(df['response'].value_counts(normalize=True))\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = {get_label(config): measure(df) for config, df in filter_by_dataset(base_dfs, dataset).items()}\n",
    "    print(f\"Entropy of object-level completions for {dataset}\")\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "    plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "    # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = {get_label(config): measure(df) for config, df in filter_by_dataset(self_pred_dfs, dataset).items()}\n",
    "    print(f\"Entropy of meta-level completions for {dataset}\")\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "    plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "    # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
