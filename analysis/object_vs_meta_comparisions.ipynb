{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing objectâ€“level completions against metaâ€“level predictions\n",
    "This notebook compares how well different models do scored against base predictions from itself or other models. This is most useful in checking finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_FOLDERS = [ # ðŸ”µ within exp/\n",
    "    \"pers_pref_test\"\n",
    "]\n",
    "    \n",
    "CONDITIONS = { \n",
    "    # see `analysis/loading_data.py` for details\n",
    "    # (\"task\", \"set\"): [\"val\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-4-1106-preview\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo-1106\", \"gpt-4-0613\"],\n",
    "    # (\"prompt\", \"method\"): [\"base-completion-bergenia\", \"self-prediction-bergenia-nontechnical\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo\", \"claude-2.1\"],\n",
    "    # (\"language_model\",\"model\"): [\"davinci-002\"],\n",
    "    # (\"dataset\", \"topic\"): [\"number_triplets\"],\n",
    "    # (\"dataset\", \"topic\"): [\"english_words\"],\n",
    "    # (\"dataset\",\"n_shot\"): [100, None]\n",
    "    # (\"dataset\",\"n_shot\"): [0, None],\n",
    "    # (\"dataset\",\"n_shot_seeding\"): [\"other_model\"]\n",
    "    # ('dataset', 'string_modifier'): ['None', None],\n",
    "    # ('dataset', 'response_property'): ['None', None],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.analysis_helpers import merge_object_and_meta_dfs, create_df_from_configs, fill_df_with_function, get_pretty_name, filter_configs_by_conditions, pretty_print_config, get_pretty_name_w_labels,  merge_object_and_meta_dfs_and_run_property_extraction\n",
    "from evals.analysis.loading_data import load_dfs_with_filter, load_base_df_from_config, get_hydra_config, load_single_df, get_data_path\n",
    "from evals.utils import get_maybe_nested_from_dict\n",
    "from evals.analysis.analysis_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\", 64)\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set font for plots\n",
    "plt.rcParams[\"font.family\"] = \"Univers Next Pro\"\n",
    "\n",
    "# retina plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seaborn to shut up\n",
    "import warnings\n",
    "# Ignore the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.locations import REPO_DIR, EXP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataframes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with configs as keys\n",
    "dfs = {}\n",
    "for STUDY_FOLDER in STUDY_FOLDERS:\n",
    "    _dfs = load_dfs_with_filter(EXP_DIR / STUDY_FOLDER, CONDITIONS, exclude_noncompliant=False)\n",
    "    dfs.update(_dfs)\n",
    "    print(f\"Loaded {len(_dfs)} dataframes from {STUDY_FOLDER}\")\n",
    "print(f\"Loaded {len(dfs)} dataframes in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base_config(config):\n",
    "    return config[\"prompt\"][\"method\"].startswith(\"object\") or config[\"prompt\"][\"method\"].startswith(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_dfs = {config: df for config, df in dfs.items() if is_base_config(config)}\n",
    "meta_dfs = {config: df for config, df in dfs.items() if not is_base_config(config)}\n",
    "print(f\"Loaded {len(object_dfs)} base and {len(meta_dfs)} self-prediction dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have the following datasets:\")\n",
    "datasets = set([get_maybe_nested_from_dict(k, ('task', 'name')) for k in object_dfs.keys()])\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have the following response properties:\")\n",
    "response_properties = set([get_maybe_nested_from_dict(k, ('response_property', 'name')) for k in meta_dfs.keys()])\n",
    "print(response_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LABELS = {\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnum:8x4lehAb\": \"GPT3.5 fted on GPT3.5\" ,\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnumscram:8x6QzXiQ\": \"GPT3.5 fted on GPT3.5\\n(scrambled)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnum:8xMcmGZM\": \"GPT3.5 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on4onnum:8x8dNwL1\": \"GPT4 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on35onnum:8xq9fNVt\": \"GPT4 fted on GPT3.5\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnums:8zFjiOFt\": \"GPT3.5 fted on GPT3.5 (small dataset)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnums:8zHmk4o8\": \"GPT3.5 fted on GPT4 (small dataset)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35nwvrp:8zJsJdOE\": \"GPT3.5 fted on GPT3.5\\n(various response properties)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:sweep:97WTZlBs\": \"GPT3.5 fted on GPT3.5\",\n",
    "    \"gpt-3.5-turbo-1106\": \"GPT3.5\",\n",
    "    \"gpt-4-0613\": \"GPT4\",\n",
    "    \"claude-3-sonnet-20240229\": \"Claude 3 Sonnet\",\n",
    "    \"claude-3-opus-20240229\": \"Claude 3 Opus\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wo_labels = [l for l in {get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()}) if l not in MODEL_LABELS]\n",
    "if len(models_wo_labels) > 0: print(\"Models without labels:\") \n",
    "else: print(\"All models have labels\")\n",
    "for m in models_wo_labels:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(config):\n",
    "    label = \"\"\n",
    "    if isinstance(config, str):\n",
    "        config = eval(config)\n",
    "    model = get_maybe_nested_from_dict(config, ('language_model', 'model'))\n",
    "    if model in MODEL_LABELS:\n",
    "        model = MODEL_LABELS[model]\n",
    "    label += model\n",
    "    response_property = get_maybe_nested_from_dict(config, ('response_property', 'name'))\n",
    "    if response_property not in [\"None\", None]:\n",
    "        label += f\"\\n predicting {response_property}\"\n",
    "    note = get_maybe_nested_from_dict(config, 'note')\n",
    "    if note not in [\"None\", None]:\n",
    "        label += f\"\\n{note}\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_mode_object_df(df: pd.DataFrame):\n",
    "    \"\"\"Takes in an object level df and returns a version where every response has been swapped out for the mode response in the dataframe. \n",
    "    This allows us to score how well the model would be at always meta-level predicting the mode. This corresponds to the model during finetuning learning to only predict the most common response, without learning any connection to the inputs\n",
    "    \"\"\"\n",
    "    # ensure that we're not changing the input df in-place\n",
    "    df = df.copy()\n",
    "    # get most common response\n",
    "    mode = df[df['compliance'] == True]['response'].mode()[0] # if multiple most common answers, chooses one\n",
    "    mode_row = df[df['response'] == mode].head(1)\n",
    "    # drop the input string\n",
    "    mode_row = mode_row.drop(\"string\", axis=1).drop(\"compliance\", axis=1)\n",
    "    # replace the rest of every row with mode_row\n",
    "    for column in mode_row.columns:\n",
    "        df[column] = [mode_row[column].item()] * len(df)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairwise_tables(measure, object_dfs, meta_dfs):\n",
    "    results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()])\n",
    "    baseline_results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()]) # we compare the model against the baseline of \n",
    "    for object_config, object_df in object_dfs.items():\n",
    "        for meta_config, meta_df in meta_dfs.items():\n",
    "            # compute joint df\n",
    "            joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                object_df,\n",
    "                meta_df,\n",
    "                object_config,\n",
    "                meta_config,\n",
    "            )\n",
    "            if len(joint_df) == 0:\n",
    "                print(f\"Empty dataframe for {object_config} and {meta_config}\")\n",
    "                continue\n",
    "            results.loc[str(meta_config), str(object_config)] = measure(joint_df)\n",
    "\n",
    "            # what would we see under the baseline of always picking the object-level mode?\n",
    "            # modify the object-level df to always contain the mode\n",
    "            mode_object_df = construct_mode_object_df(object_df)\n",
    "            # compute joint df\n",
    "            mode_joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                mode_object_df,\n",
    "                meta_df,\n",
    "                object_config,\n",
    "                meta_config,\n",
    "            )\n",
    "            if len(joint_df) == 0:\n",
    "                continue\n",
    "            baseline_results.loc[str(meta_config), str(object_config)] = measure(mode_joint_df)\n",
    "    results.index = results.index.map(get_label)\n",
    "    results.columns = results.columns.map(get_label)\n",
    "    # do we have columns that are all NaN? This happens when we are reading in task.set==train dataframes, and only compare against val\n",
    "    results = results.dropna(axis=1, how='all')\n",
    "    # do we have rows that are all NaN?\n",
    "    results = results.dropna(axis=0, how='all')\n",
    "    # sort the columns and the rows\n",
    "    results = results.sort_index(axis=0)\n",
    "    results = results.sort_index(axis=1)\n",
    "    # the saem for the baseline results\n",
    "    baseline_results.index = baseline_results.index.map(get_label)\n",
    "    baseline_results.columns = baseline_results.columns.map(get_label)\n",
    "    # do we have columns that are all NaN? This happens when we are reading in task.set==train dataframes, and only compare against val\n",
    "    baseline_results = baseline_results.dropna(axis=1, how='all')\n",
    "    # do we have rows that are all NaN?\n",
    "    baseline_results = baseline_results.dropna(axis=0, how='all')\n",
    "    # sort the columns and the rows\n",
    "    baseline_results = baseline_results.sort_index(axis=0)\n",
    "    baseline_results = baseline_results.sort_index(axis=1)\n",
    "    return results, baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_dataset(dfs, dataset):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset}\n",
    "\n",
    "def filter_by_dataset_and_response_property(dfs, dataset, response_property):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset and get_maybe_nested_from_dict(config, ('response_property', 'name')) == response_property}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        results, baseline_results = make_pairwise_tables(calc_accuracy_with_excluded, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "        print(f\"Accuracy for {dataset}\")\n",
    "        if len(results) == 0 or results.shape[0] == 0:\n",
    "            print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.heatmap(results.astype(float), cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, annot=True, fmt=\".2f\", ax=ax)\n",
    "\n",
    "        # Add baseline_results as light grey annotations\n",
    "        for text, baseline_result in zip(ax.texts, (baseline_results.values.flatten())):\n",
    "            text.set_text(f\"{text.get_text()}\\n({baseline_result:.2f})\")\n",
    "\n",
    "        # add text explaining the baseline\n",
    "        ax.text(-0.2, -0.1, \"Baseline\\n(predicting the mode)\\nin parentheses\", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, color=\"grey\")\n",
    "\n",
    "        ax.set_xlabel(\"Scored against object-level\")\n",
    "        ax.set_ylabel(\"Meta-level\")\n",
    "        ax.set_title(f\"Accuracy of meta-level predicting object-level models\\non {dataset} eliciting {response_property}\")\n",
    "        ax.set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logprob heatmap\n",
    "What is the logprob of the _first token_ of the correct answer under the metaâ€“level model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        results, baseline_results = make_pairwise_tables(likelihood_of_correct_first_token, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "        print(f\"Accuracy for {dataset}\")\n",
    "        if len(results) == 0 or results.shape[0] == 0:\n",
    "            print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "        sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False)\n",
    "        plt.xlabel(\"Scored against object-level\")\n",
    "        plt.ylabel(\"Meta-level\")\n",
    "        plt.title(f\"Mean log-prob of initial object-level response under meta-level model\\non {dataset} eliciting {response_property}\")\n",
    "        plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object vs object change heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    results, baseline_results = make_pairwise_tables(calc_accuracy, filter_by_dataset(object_dfs, dataset), filter_by_dataset(object_dfs, dataset))\n",
    "    print(f\"Overlap between object-level completions for {dataset}\")\n",
    "    \n",
    "    mask = np.triu(np.ones_like(results, dtype=bool), k=1)\n",
    "    sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, fmt=\".0%\", mask=mask)\n",
    "    # plt.xlabel(\"Scored against object-level\")\n",
    "    # plt.ylabel(\"Meta-level\")\n",
    "    plt.title(f\"Overlap between object-level completions for {dataset}\")\n",
    "    plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: stats.entropy(df['response'].value_counts(normalize=True))\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = {get_label(config): measure(df) for config, df in filter_by_dataset(object_dfs, dataset).items()}\n",
    "    print(f\"Entropy of object-level completions for {dataset}\")\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "    plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "    # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = {get_label(config): measure(df) for config, df in filter_by_dataset(meta_dfs, dataset).items()}\n",
    "    print(f\"Entropy of meta-level completions for {dataset}\")\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "    plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "    # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: (df['compliance'] == True).mean()\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = {get_label(config): measure(df) for config, df in filter_by_dataset(object_dfs, dataset).items()}\n",
    "    print(f\"Compliance of object-level completions for {dataset}\")\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "    plt.title(f\"Compliance of object-level completions for {dataset}\")\n",
    "    # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.xticks(rotation=90)\n",
    "    # scale to percent\n",
    "    plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "    plt.show()\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = {get_label(config): measure(df) for config, df in filter_by_dataset(meta_dfs, dataset).items()}\n",
    "    print(f\"Compliance of meta-level completions for {dataset}\")\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "    plt.title(f\"Compliance of object-level completions for {dataset}\")\n",
    "    # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.xticks(rotation=90)\n",
    "    # scale to percent\n",
    "    plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
