{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing objectâ€“level completions against metaâ€“level predictions\n",
    "This notebook compares how well different models do scored against base predictions from itself or other models. This is most useful in checking finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_FOLDERS = [ # ðŸ”µ within exp/\n",
    "    \"second_run_tests\",\n",
    "]\n",
    "    \n",
    "CONDITIONS = { \n",
    "    # see `analysis/loading_data.py` for details\n",
    "    # (\"language_model\",\"model\"): [\"gpt-4-1106-preview\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo-1106\", \"gpt-4-0613\"],\n",
    "    # (\"prompt\", \"method\"): [\"base-completion-bergenia\", \"self-prediction-bergenia-nontechnical\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo\", \"claude-2.1\"],\n",
    "    # (\"language_model\",\"model\"): [\"davinci-002\"],\n",
    "    # (\"dataset\", \"topic\"): [\"number_triplets\"],\n",
    "    # (\"dataset\", \"topic\"): [\"english_words\"],\n",
    "    # (\"dataset\",\"n_shot\"): [100, None]\n",
    "    (\"dataset\",\"n_shot\"): [0, None],\n",
    "    # (\"dataset\",\"n_shot_seeding\"): [\"other_model\"]\n",
    "    # ('dataset', 'string_modifier'): ['None', None],\n",
    "    # ('dataset', 'response_property'): ['None', None],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.analysis_helpers import merge_object_and_meta_dfs, create_df_from_configs, fill_df_with_function, get_pretty_name, filter_configs_by_conditions, pretty_print_config, get_pretty_name_w_labels,  merge_object_and_meta_dfs_and_run_property_extraction\n",
    "from evals.analysis.loading_data import load_dfs_with_filter, load_base_df_from_config, get_hydra_config, load_single_df, get_data_path\n",
    "from evals.utils import get_maybe_nested_from_dict\n",
    "from evals.analysis.analysis_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\", 64)\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set font for plots\n",
    "plt.rcParams[\"font.family\"] = \"Univers Next Pro\"\n",
    "\n",
    "# retina plots\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seaborn to shut up\n",
    "import warnings\n",
    "# Ignore the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.generate_few_shot import REPO_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "EXPDIR = Path(REPO_DIR) / \"exp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataframes in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with configs as keys\n",
    "dfs = {}\n",
    "for STUDY_FOLDER in STUDY_FOLDERS:\n",
    "    _dfs = load_dfs_with_filter(EXPDIR / STUDY_FOLDER, CONDITIONS, exclude_noncompliant=False)\n",
    "    dfs.update(_dfs)\n",
    "    print(f\"Loaded {len(_dfs)} dataframes from {STUDY_FOLDER}\")\n",
    "print(f\"Loaded {len(dfs)} dataframes in total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base_config(config):\n",
    "    return config[\"prompt\"][\"method\"].startswith(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_dfs = {config: df for config, df in dfs.items() if is_base_config(config)}\n",
    "meta_dfs = {config: df for config, df in dfs.items() if not is_base_config(config)}\n",
    "print(f\"Loaded {len(object_dfs)} base and {len(meta_dfs)} self-prediction dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have the following datasets:\")\n",
    "datasets = set([get_maybe_nested_from_dict(k, ('task', 'name')) for k in object_dfs.keys()])\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have the following response properties:\")\n",
    "response_properties = set([get_maybe_nested_from_dict(k, ('response_property', 'name')) for k in meta_dfs.keys()])\n",
    "print(response_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LABELS = {\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnum:8x4lehAb\": \"GPT3.5 fted on GPT3.5\" ,\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnumscram:8x6QzXiQ\": \"GPT3.5 fted on GPT3.5\\n(scrambled)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnum:8xMcmGZM\": \"GPT3.5 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on4onnum:8x8dNwL1\": \"GPT4 fted on GPT4\",\n",
    "    \"ft:gpt-4-0613:dcevals-kokotajlo:4on35onnum:8xq9fNVt\": \"GPT4 fted on GPT3.5\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35onnums:8zFjiOFt\": \"GPT3.5 fted on GPT3.5 (small dataset)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on4onnums:8zHmk4o8\": \"GPT3.5 fted on GPT4 (small dataset)\",\n",
    "    \"ft:gpt-3.5-turbo-1106:dcevals-kokotajlo:35on35nwvrp:8zJsJdOE\": \"GPT3.5 fted on GPT3.5\\n(various response properties)\",\n",
    "    \"gpt-3.5-turbo-1106\": \"GPT3.5\",\n",
    "    \"gpt-4-0613\": \"GPT4\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_wo_labels = [l for l in {get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in object_dfs.keys()}.union({get_maybe_nested_from_dict(c, ('language_model', 'model')) for c in meta_dfs.keys()}) if l not in MODEL_LABELS]\n",
    "if len(models_wo_labels) > 0: print(\"Models without labels:\") \n",
    "else: print(\"All models have labels\")\n",
    "for m in models_wo_labels:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(config):\n",
    "    label = \"\"\n",
    "    if isinstance(config, str):\n",
    "        config = eval(config)\n",
    "    model = get_maybe_nested_from_dict(config, ('language_model', 'model'))\n",
    "    if model in MODEL_LABELS:\n",
    "        model = MODEL_LABELS[model]\n",
    "    label += model\n",
    "    response_property = get_maybe_nested_from_dict(config, ('response_property', 'name'))\n",
    "    if response_property not in [\"None\", None]:\n",
    "        label += f\"\\n predicting {response_property}\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairwise_tables(measure, object_dfs, meta_dfs):\n",
    "    results = pd.DataFrame(columns=[str(config) for config in object_dfs.keys()], index=[str(config) for config in meta_dfs.keys()])\n",
    "    for object_config, object_df in object_dfs.items():\n",
    "        for meta_config, meta_df in meta_dfs.items():\n",
    "            joint_df = merge_object_and_meta_dfs_and_run_property_extraction(\n",
    "                object_df,\n",
    "                meta_df,\n",
    "                object_config,\n",
    "                meta_config,\n",
    "            )\n",
    "            results.loc[str(meta_config), str(object_config)] = measure(joint_df)\n",
    "    results.index = results.index.map(get_label)\n",
    "    results.columns = results.columns.map(get_label)\n",
    "    # sort the columns and the rows\n",
    "    results = results.sort_index(axis=0)\n",
    "    results = results.sort_index(axis=1)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_dataset(dfs, dataset):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset}\n",
    "\n",
    "def filter_by_dataset_and_response_property(dfs, dataset, response_property):\n",
    "    return {config: df for config, df in dfs.items() if get_maybe_nested_from_dict(config, ('task', 'name')) == dataset and get_maybe_nested_from_dict(config, ('response_property', 'name')) == response_property}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for response_property in response_properties:\n",
    "        results = make_pairwise_tables(calc_accuracy_with_excluded, filter_by_dataset(object_dfs, dataset), filter_by_dataset_and_response_property(meta_dfs, dataset, response_property))\n",
    "        print(f\"Accuracy for {dataset}\")\n",
    "        if len(results) == 0 or results.shape[0] == 0:\n",
    "            print(f\"No data for {dataset} / {response_property}\")\n",
    "            continue\n",
    "        sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, fmt=\".0%\")\n",
    "        plt.xlabel(\"Scored against object-level\")\n",
    "        plt.ylabel(\"Meta-level\")\n",
    "        plt.title(f\"Accuracy of meta-level predicting object-level models\\non {dataset} eliciting {response_property}\")\n",
    "        plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base vs base change heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    results = make_pairwise_tables(calc_accuracy, filter_by_dataset(object_dfs, dataset), filter_by_dataset(object_dfs, dataset))\n",
    "    print(f\"Overlap between object-level completions for {dataset}\")\n",
    "    \n",
    "    mask = np.triu(np.ones_like(results, dtype=bool), k=1)\n",
    "    sns.heatmap(results.astype(float), annot=True, cmap=\"YlGnBu\", cbar=False, vmin=0, vmax=1, fmt=\".0%\", mask=mask)\n",
    "    # plt.xlabel(\"Scored against object-level\")\n",
    "    # plt.ylabel(\"Meta-level\")\n",
    "    plt.title(f\"Overlap between object-level completions for {dataset}\")\n",
    "    plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure = lambda df: stats.entropy(df['response'].value_counts(normalize=True))\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = {get_label(config): measure(df) for config, df in filter_by_dataset(object_dfs, dataset).items()}\n",
    "    print(f\"Entropy of object-level completions for {dataset}\")\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"green\")\n",
    "\n",
    "    plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "    # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = {get_label(config): measure(df) for config, df in filter_by_dataset(meta_dfs, dataset).items()}\n",
    "    print(f\"Entropy of meta-level completions for {dataset}\")\n",
    "    sns.barplot(x=list(results.keys()), y=list(results.values()), color = \"purple\")\n",
    "\n",
    "    plt.title(f\"Entropy of object-level completions for {dataset}\")\n",
    "    # plt.gca().set_aspect(\"equal\")  # Set aspect ratio to \"equal\" for square cells\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
