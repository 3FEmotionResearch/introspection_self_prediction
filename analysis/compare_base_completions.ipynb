{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing base completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [ # inside of exp/\n",
    "    \"how_different_are_GPT35_versions\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set log level\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.analysis.analysis_helpers import merge_object_and_meta_dfs, create_df_from_configs, fill_df_with_function, get_pretty_name, filter_configs_by_conditions, pretty_print_config, get_pretty_name_w_labels\n",
    "from evals.analysis.loading_data import load_dfs_with_filter, load_base_df_from_config, get_hydra_config, load_single_df, get_data_path\n",
    "from evals.utils import get_maybe_nested_from_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\")\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seaborn to shut up\n",
    "import warnings\n",
    "# Ignore the specific FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.locations import EXP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "EXPDIR = EXP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with configs as keys\n",
    "dfs = {}\n",
    "for exp_folder in EXP_FOLDERS:\n",
    "    _dfs = load_dfs_with_filter(EXPDIR / exp_folder, conditions={}, exclude_noncompliant=False)\n",
    "    print(f\"Loaded {len(_dfs)} dataframes from {exp_folder}\")\n",
    "    dfs.update(_dfs)\n",
    "print(f\"Loaded {len(dfs)} dataframes in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyses\n",
    "Create results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results dataframe\n",
    "results = create_df_from_configs(dfs.keys())\n",
    "results.sort_values(by=[\"language_model_model\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a grouping column\n",
    "\n",
    "config_vals_of_interest = [\n",
    "    [\"language_model\", \"model\"],\n",
    "    \"note\",\n",
    "    # [\"prompt\", \"method\"],\n",
    "    # \"base_dir\",\n",
    "    # \"exp_dir\",\n",
    "    # \"limit\",\n",
    "    # \"dataset\",\n",
    "    # [\"dataset\", \"topic\"],\n",
    "    # [\"dataset\", \"n_shot\"],\n",
    "    # [\"dataset\", \"n_shot_seeding\"],\n",
    "    [\"dataset\", \"string_modifier\"],\n",
    "    [\"dataset\", \"response_property\"],\n",
    "    \"prediction_target\",\n",
    "]\n",
    "\n",
    "results[\"grouping\"] = results[\"config\"].apply(lambda x: get_pretty_name_w_labels(x, config_vals_of_interest))\n",
    "\n",
    "print(f\"Got {results.grouping.nunique()} unique groupings for {len(results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would like to know how likely the model is to give the correct answer. However, the Chat API does not allow us to get the likelihood of a given response, so we use the likelihood of the first token as a proxy. If the correct response is not in the list of top logprobs, we assume the likelihood is flat over all other tokens, which our token is in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many strings are correctly produced by the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_noncompliant(df):\n",
    "    df = df.copy()\n",
    "    df = df[(df['compliance_self'] == True) & (df['compliance_base'] == True)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much variance is there in the responses? Calculate Shannon entropy over responses\n",
    "def calc_entropy(df, col):\n",
    "    \"\"\"Calculate the entropy of the model\"\"\"\n",
    "    return stats.entropy(df[col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also check if the model is following some cheap strategy\n",
    "fill_df_with_function(dfs, lambda df: df['last_word_repeated'].mean(), \"last_word_repeated\", results)\n",
    "fill_df_with_function(dfs, lambda df: df['last_char_repeated'].mean(), \"last_char_repeated\", results)\n",
    "fill_df_with_function(dfs, lambda df: df['any_word_repeated'].mean(), \"any_word_repeated\", results)\n",
    "fill_df_with_function(dfs, lambda df: calc_entropy(df, \"response\"), \"entropy\", results)\n",
    "fill_df_with_function(dfs, lambda df: np.mean(df['response'].str.lower() == df['target'].str.lower()), \"correct\", results)\n",
    "fill_df_with_function(dfs, lambda df: np.mean(df['compliance'] == True), \"compliance\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_OF_INTEREST = [\n",
    "    \"correct\",\n",
    "    'compliance',\n",
    "    \"entropy\",\n",
    "    \"last_word_repeated\",\n",
    "    \"last_char_repeated\",\n",
    "    \"any_word_repeated\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in COLS_OF_INTEREST:    \n",
    "    sns.barplot(data=results, x=\"language_model_model\", y=col, hue=\"grouping\")\n",
    "    plt.title(col.capitalize())\n",
    "    plt.xlabel(\"Few-shot n\")\n",
    "    plt.ylabel(col.capitalize())\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise comparisons between dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pair_dfs = {}\n",
    "for i, (key, df) in enumerate(dfs.items()):\n",
    "    for j, (key2, df2) in enumerate(dfs.items()):\n",
    "        if i < j:  # Only merge if the index of the first key is less than the index of the second key\n",
    "            merged_pair_dfs[(key, key2)] = merge_object_and_meta_dfs(df, df2)\n",
    "\n",
    "print(f\"Merged {len(merged_pair_dfs)} dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often do the models give the same response?\n",
    "for (key, key2), df in merged_pair_dfs.items():\n",
    "    pretty_print_config(key)\n",
    "    print(\"vs\")\n",
    "    pretty_print_config(key2)\n",
    "    print(f\"Same response: {np.mean(df['response_object'] == df['response_meta']) * 100:.2f}%\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed analysis\n",
    "\n",
    "Name conditions, and it will pull the relevant dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_conditions = { \n",
    "    # (\"language_model\",\"model\"): [\"gpt-3.5-turbo-0125\"],\n",
    "    # (\"language_model\",\"model\"): [\"claude-2.1\"],\n",
    "    # (\"language_model\",\"model\"): [\"davinci-002\"],\n",
    "    # (\"language_model\",\"model\"): [\"gpt-4-1106-preview\"],\n",
    "    # (\"dataset\",\"n_shot\"): [9], \n",
    "    # (\"prediction_target\"): [\"self\"],\n",
    "    # (\"dataset\",\"n_shot_seeding\"): [True]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_configs = filter_configs_by_conditions(dfs.keys(), filter_conditions)\n",
    "print(f\"Got {len(filtered_configs)}, down from {len(dfs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in filtered_configs: pretty_print_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dfs\n",
    "filtered_merged_dfs = {config: df for config, df in dfs.items() if config in filtered_configs}\n",
    "print(f\"Got {len(filtered_merged_dfs)}, down from {len(dfs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    display(detail_df.sample(5))\n",
    "    # display(detail_df[[\"string\", \"response_base\", \"response_self\", \"raw_response_self\",  'few-shot_string', 'few-shot_response']].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    # what are the most common base predictions pairs?\n",
    "    display(detail_df[[\"response\"]].value_counts(normalize=True).head(10) * 100)\n",
    "\n",
    "    # Filter out non-numeric values\n",
    "    detail_df['response_numeric'] = detail_df['response'].apply(lambda x: int(x) if x.isnumeric() else None)\n",
    "    detail_df = detail_df.dropna(subset=[\"response_numeric\"])\n",
    "    detail_df['response_numeric'].hist(bins=100)\n",
    "    plt.title(\"Distribution of base predictions\")\n",
    "    plt.xlabel(\"Prediction\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config,detail_df in filtered_merged_dfs.items():\n",
    "    pretty_print_config(config=config)\n",
    "    display(detail_df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
