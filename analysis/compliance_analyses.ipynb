{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compliance Checks\n",
    "To what degree do the models do or refuse their tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the experiment with the base completions we want to use?\n",
    "STUDY_FOLDER = \"english_words_basic\" # ðŸ”µ within exp/\n",
    "CONDITIONS = { \n",
    "    # see `analysis/loading_data.py` for details\n",
    "    # \"language_model\": [\"gpt-3.5-turbo\", \"gpt-4-turbo\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the git command to get the repository root directory\n",
    "REPO_DIR = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"]).decode().strip()\n",
    "\n",
    "print(\"Repository directory:\", REPO_DIR)\n",
    "sys.path.append(REPO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import words\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_helpers import merge_base_and_self_pred_dfs, create_df_from_configs, fill_df_with_function\n",
    "from loading_data import load_dfs_with_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to None to show all content\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set color palette\n",
    "palette = sns.color_palette(\"Set1\")\n",
    "sns.set_palette(palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for the data\n",
    "EXPDIR = Path(REPO_DIR) / \"exp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframes with configs as keys while keeping the non-compliant ones\n",
    "dfs = load_dfs_with_filter(EXPDIR / STUDY_FOLDER, CONDITIONS, exclude_noncompliant=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_base_config(config):\n",
    "    return \"base\" in config[\"prompt\"][\"method\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dfs = {config: df for config, df in dfs.items() if is_base_config(config)}\n",
    "self_pred_dfs = {config: df for config, df in dfs.items() if not is_base_config(config)}\n",
    "print(f\"Loaded {len(base_dfs)} base and {len(self_pred_dfs)} self-prediction dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create results dataframe\n",
    "results = create_df_from_configs(dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many responses are non-compliant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_compliance(df):\n",
    "    return (df[\"compliance\"] == True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the results dataframe with the accuracy and t-statistic\n",
    "fill_df_with_function(dfs, avg_compliance, \"avg_compliance\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.drop(columns=[\"config\"]).sort_values(by=\"avg_compliance\", ascending=False).style.hide(axis=\"index\").background_gradient(subset=[\"avg_compliance\"], cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(data=results, x=\"dataset_n_shot\", y=\"avg_compliance\", hue=\"language_model\")\n",
    "plt.title(\"Average compliance per number of few-shot examples\")\n",
    "plt.xlabel(\"Number of few-shot examples\")\n",
    "plt.ylabel(\"Average compliance\")\n",
    "plt.ylim(-0.025, 1.025)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bar plot with bars for each language model and prompt method and also n_shot\n",
    "sns.barplot(data=results, x=\"language_model\", y=\"avg_compliance\", hue=\"prompt_method\")\n",
    "plt.title(\"Average compliance per language model and prompt method\")\n",
    "plt.xlabel(\"Language model\")\n",
    "plt.ylabel(\"Average compliance\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
